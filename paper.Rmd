---
title: "Automated Assessment of Residual Plots with Computer Vision Models"
author:
  - name: Weihao Li
    affil: a, b
    email: patrick.li@anu.edu.au
  - name: Dianne Cook
    affil: a
    email: dicook@monash.edu
  - name: Emi Tanaka
    affil: a, b, c
    email: emi.tanaka@anu.edu.au
  - name: Susan VanderPlas
    affil: d
    email: susan.vanderplas@unl.edu
  - name: Klaus Ackermann
    affil: a
    email: klaus.ackermann@monash.edu 
affiliation:
  - num: a
    address: |
      Department of Econometrics and Business Statistics, Monash University, Clayton, VIC, Australia
  - num: b
    address: |
      Biological Data Science Institute, Australian National University, Acton, ACT, Australia
  - num: c
    address: |
      Research School of Finance, Actuarial Studies and Statistics, Australian National University, Acton, ACT, Australia
  - num: d
    address: |
      Department of Statistics, University of Nebraska, Lincoln, Nebraska, USA
bibliography: bibliography.bib
abstract: |
  Plotting the residuals is a recommended procedure to diagnose deviations from linear model assumptions, such as non-linearity, heteroscedasticity, and non-normality. The presence of structure in residual plots can be tested using the lineup protocol to do visual inference. There are a variety of conventional residual tests, but the lineup protocol, used as a statistical test, performs better for diagnostic purposes because it is less sensitive and applies more broadly to different types of departures. However, the lineup protocol relies on human judgment which limits its scalability. This work presents a solution by providing a computer vision model to automate the assessment of residual plots. It is trained to predict a distance measure that quantifies the disparity between the residual distribution of a fitted classical normal linear regression model and the reference distribution, based on Kullback-Leibler divergence. From extensive simulation studies, the computer vision model exhibits lower sensitivity than conventional tests but higher sensitivity than human visual tests. It is slightly less effective on non-linearity patterns. Several examples from classical papers and contemporary data illustrate the new procedures, highlighting its usefulness in automating the diagnostic process and supplementing existing methods.
keywords: |
  statistical graphics; data visualization; visual inference; computer vision; machine learning; hypothesis testing; reression analysis; cognitive perception; simulation; practical significance
header-includes: |
  \usepackage{lscape}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage{setspace}
  \doublespacing
output: rticles::tf_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "100%", 
  fig.align = "center")
```

```{r}
# Visual inference models and p-value calculation
# remotes::install_github("TengMCing/visage")
options(tinytex.clean = FALSE)
library(tidyverse)
library(visage)
library(glue)

# To control the simulation in this file
set.seed(10086)
```

# Introduction {#sec-model-introduction}

Plotting residuals is commonly regarded as a standard practice in linear regression diagnostics [@belsley1980regression; @cook1982residuals]. This visual assessment plays a crucial role in identifying whether model assumptions, such as linearity, homoscedasticity, and normality, are reasonable. It also helps in understanding the goodness of fit and various unexpected characteristics of the model.

Generating a residual plot in most statistical software is often as straightforward as executing a line of code or clicking a button. However, accurately interpreting a residual plot can be challenging. A residual plot can exhibit various visual features, but it is crucial to recognize that some may arise from the characteristics of predictors and the natural stochastic variation of the observational unit, rather than indicating a violation of model assumptions [@li2024plot]. Consider Figure \ref{fig:false-finding} as an example, the residual plot displays a triangular left-pointing shape. The distinct difference in the spread of the residuals across the fitted values may result in the analyst suggesting that there may be heteroskedasticity, however, it is important to avoid over-interpreting this visual pattern. In this case, the fitted regression model is correctly specified, and the triangular shape is actually a result of the skewed distribution of the predictors, rather than indicating a flaw in the model.

The concept of visual inference, as proposed by @buja2009statistical, provides an inferential framework to assess whether residual plots indeed contain visual patterns inconsistent with the model assumptions. The fundamental idea involves testing whether the true residual plot visually differs significantly from null plots, where null plots are plotted with residuals generated from the residual rotation distribution [@langsrud2005rotation], which is a distribution consistent with the null hypothesis $H_0$ that the linear regression model is correctly specified. Typically, the visual test is accomplished through the lineup protocol, where the true residual plot is embedded within a lineup alongside several null plots. If the true residual plot can be distinguished from the lineup, it provides evidence for rejecting $H_0$.

The practice of delivering a residual plot as a lineup is generally regarded as a valuable approach. Beyond its application in residual diagnostics, the lineup protocol has been integrated into the analysis of diverse subjects. For instance, Loy and Hofmann [-@loy2013diagnostic; -@loy2014hlmdiag; -@loy2015you] illustrated its applicability in diagnosing hierarchical linear models. Additionally, @widen2016graphical and @fieberg2024using demonstrated its utility in geographical and ecology research respectively, while @krishnan2021hierarchical explored its effectiveness in forensic examinations.

A practical limitation of the lineup protocol lies in its reliance on human judgements [see @li2024plot about the practical limitations]. Unlike conventional statistical tests that can be performed computationally in statistical software, the lineup protocol requires human evaluation of images. This characteristic makes it less suitable for large-scale applications, given the associated high labour costs and time requirements. There is a substantial need to develop an approach to substitute these human judgement with an automated reading of data plots using machines.

The utilization of computers to interpret data plots has a rich history, with early efforts such as "Scagnostics" by @tukey1985computer, a set of numerical statistics that summarize features of scatter plots. @wilkinson2005graph expanded on this work, introducing scagnostics based on computable measures applied to planar proximity graphs. These measures, including, but not limited to, "Outlying", "Skinny", "Stringy", "Straight", "Monotonic", "Skewed", "Clumpy", and "Striated",  aimed to characterize outliers, shape, density, trend, coherence and other characteristics of the data. While this approach has been inspiring, there is a recognition [@buja2009statistical] that it may not capture all the necessary visual features that differentiate true residual plots from null plots. A more promising alternative entails enabling machines to learn the function for extracting visual features from residual plots. Essentially, this means empowering computers to discern the crucial visual features for residual diagnostics and determining the method to extract them. 

Modern computer vision models are well-suited for addressing this challenge. They rely on deep neural networks with convolutional layers [@fukushima1982neocognitron]. These layers use small, sliding windows to scan the image, performing a dot product to extract local features and patterns. Numerous studies have demonstrated the efficacy of convolutional layers in addressing various vision tasks, including image recognition [@rawat2017deep]. Despite the widespread use of computer vision models in fields like computer-aided diagnosis [@lee2015image], pedestrian detection [@brunetti2018computer], and facial recognition [@emami2012facial], their application in reading data plots remains limited. While some studies have explored the use of computer vision models for tasks such as reading recurrence plots for time series regression [@ojeda2020multivariate], time series classification [@chu2019automatic; @hailesilassie2019financial; @hatami2018classification; @zhang2020encoding], anomaly detection [@chen2020convolutional], and pairwise causality analysis [@singh2017deep], the application of reading residual plots with computer vision models is a new field of study.

In this paper, we develop computer vision models and integrate them into the residual plots diagnostics workflow, addressing the need for an automated visual inference. The paper is structured as follows. Section \ref{sec-model-specifications} discusses various specifications of the computer vision models. Section \ref{sec-model-distance-between-residual-plots} defines the distance measure used to detect model violations, while Section \ref{sec-model-distance-estimation} explains how the computer vision models estimate this distance measure. Section \ref{sec-model-statistical-testing} covers the statistical tests based on the estimated distance, and Section \ref{sec-model-violations-index} introduces a Model Violations Index, which offers a quicker and more convenient assessment. Sections \ref{sec-model-architecture} and \ref{sec-model-data-generation} describe the model architecture and the corresponding data generation and training process, respectively. The results are presented in Section \ref{sec-model-results}. Example dataset applications are discussed in Section \ref{sec-examples}. Finally, we conclude with a discussion of our findings and propose ideas for future research directions.



```{r false-finding, fig.pos="!h", fig.width = 4, fig.height = 4*4/5, fig.pos="!h", fig.cap="An example residual vs fitted values plot (red line indicates 0 corresponds to the x-intercept, i.e. $y=0$). The vertical spread of the data points varies with the fitted values. This often indicates the existence of heteroskedasticity, however, here the result is due to skewed distribution of the predictors rather than heteroskedasticity. The Breusch-Pagan test rejects this residual plot at 95\\% significance level ($p\\text{-value} = 0.046$)."}
set.seed(452)
ori_x <- rand_lognormal()
mod <- heter_model(b = 0, x = closed_form(~-ori_x))
ori_dat <- mod$gen(300)

ori_dat %>%
  VI_MODEL$plot(theme = theme_light(), remove_grid_line = TRUE) +
  xlab("Fitted values") +
  ylab("Residuals")
```



# Model Specifications {#sec-model-specifications}

There are various specifications of the computer vision model that can be used to assess residual plots. We discuss these specifications below focusing on two key components of the model formula: the input and the output format.

## Input Formats

Deep learning models are sensitive to input design, and several alternatives are available for encoding residual plots.

A simple approach involves feeding vectors of residuals and fitted values into the model. While this format contains all relevant information, the input length varies with sample size, which conflicts with the fixed-shape requirements of modern vision models such as those implemented in TensorFlow [@abadi2016tensorflow]. Padding can be used to enforce uniform length but risks truncation if inputs exceed the fixed size. Alternatively, fixed-size sampling of residual–fitted value pairs can ensure consistency, though at the cost of some information loss.

Another strategy is to convert residual plots into images. This enables the use of standard convolutional architectures such as VGG16 [@simonyan2014very], which can effectively capture spatial patterns. Although discretization introduces some losses in detail, image-based inputs offer a richer representation by preserving the joint distribution of residuals and fitted values in a visually interpretable format. To ensure model generalization, it is essential to use residual plots in a consistent style, i.e., same aesthetics, layout, scaling and color schemes.

Using multiple residual plots as input, such as pairs, triplets, or full lineups is also a possible option. For instance, triplet networks [@chopra2005learning] can assess visual similarity if we assume null plots are more similar from each other than from the true residual plot, which may help distinguish null plots from true residual plots. However, these architectures introduce complexity in training due to weight sharing and specialized loss functions. We experimented with this setting in a pilot study, and found multiple residual plots led to high input resolution, high computational cost and suboptimal model performance.

Balancing accuracy, interpretability, and implementation cost, we adopt a single residual plot in image format as the model input.

## Output Formats

Given the input is a single fixed-resolution image, the model output can be defined for various tasks, including binary classification, multiclass classification, and numeric regression.

In the binary case, the outcome may indicate whether the input image is consistent with a null plot, as determined either by (1) the data-generating process or (2) the outcome of a visual test based on human judgment. The first approach models the probability that a residual plot is not a null plot and is a viable option. In contrast, the second relies on experimental data that are often scarce or inconsistent across studies.

Alternatively, a multiclass outcome may classify images into categories such as "contains outliers" vs. "does not contain outliers" or "is non-linear" vs. "not non-linear," among other diagnostic features. While this approach is appealing in theory, it essentially reverts to the traditional testing framework, where tests focus on detecting specific types of model violations in isolation.

A third option is to produce a meaninful and interpretable numerical measure that quantifies the strength of suspicious visual patterns reflecting the extent of model violations, or the difficulty index for identifying whether a residual plot has no issues. However, such measures are often used informally in daily communications but rarely formalized. For supervised learning, they must be clearly defined and reliably estimable.

In this study, we defined and used a continuous distance between a true residual plot and a theoretically "good" residual plot. This approach captures the degree to which a plot deviates from model assumptions. In contrast, a probability output based on binary labels, where true residual plots are labeled as 1 and null plots as 0, cannot express the severity of violation. It forces the model to infer this information solely from the data distribution, which may be inefficient and discards useful prior knowledge.


# Distance from a Theoretically "Good" Residual Plot  {#sec-model-distance-between-residual-plots}

To develop a computer vision model for assessing residual plots within the visual inference framework, it is important to precisely define a numerical measure of "difference" or "distance" between plots. This distance can take the form of a basic statistical operation on pixels, such as the sum of square differences, however, a pixel-to-pixel comparison makes little sense in comparing residual plots where the main interest would be structural patterns. Alternatively, it could involve established image similarity metrics like the Structural Similarity Index Measure [@wang2004image] which compares images by integrating three perception features of an image: contrast, luminance, and structure (related to average, standard deviation and correlation of pixel values over a window, respectively). These image similarity metrics are tailored for image comparison in vastly different tasks to evaluating data plots, where only essential plot elements require assessment [@chowdhury2018measuring]. We can alternatively define a notion of distance by integrating key plot elements (instead of key perception features like luminance, contrast, and structure), such as those captured by scagnostics mentioned in Section \ref{sec-model-introduction}, but the functional form still needs to be carefully refined to accurately reflect the extent of the violations.

In this section, we introduce a distance measure between a true residual plot and a theoretically 'good' residual plot. This measure quantifies the divergence between the residual distribution of a given fitted regression model and that of a correctly specified model. The computation assumes knowledge of the data generating processes for predictors and response variables. Since these processes are often unknown in practice, we will discuss a method to estimate this distance using a computer vision model in Section \ref{sec-model-distance-estimation}.

## Residual Distribution

For a classical normal linear regression model, $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{e}$, the residual $\hat{\boldsymbol{e}}$ are derived as the difference of the fitted values and observed values $\boldsymbol{y}$. Suppose the data generating process is known and the regression model is correctly specified, by the Frisch-Waugh-Lowell theorem [@frisch1933partial], residuals $\hat{\boldsymbol{e}}$ can also be treated as random variables and written as a linear transformation of the error $\boldsymbol{e}$ formulated as $\hat{\boldsymbol{e}} = \boldsymbol{R}\boldsymbol{e}$, where $\boldsymbol{R}=\boldsymbol{I}_n -\boldsymbol{X}(\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{X}^\top$ is the residual operator, $\boldsymbol{I}_n$ is a $n$ by $n$ identity matrix, and $n$ is the number of observations.

One of the assumptions of the classical normal linear regression model is that the error $\boldsymbol{e}$ follows a multivariate normal distribution with zero mean and constant variance, i.e., $\boldsymbol{e} \sim N(\boldsymbol{0}_n,\sigma^2\boldsymbol{I}_n)$. It follows that the distribution of residuals $\hat{\boldsymbol{e}}$ can be characterized by a certain probability distribution, denoted as $Q$, which is transformed from the multivariate normal distribution. This reference distribution $Q$ summarizes what "good" residuals should follow given the design matrix $\boldsymbol{X}$ is known and fixed.

Suppose the design matrix $\boldsymbol{X}$ has linearly independent columns, the trace of the hat matrix $\boldsymbol{H} = \boldsymbol{X}(\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{X}^\top$ will equal to the number of columns in $\boldsymbol{X}$ denoted as $k$. As a result, the rank of $\boldsymbol{R}$ is $n - k$, and $Q$ is a degenerate multivariate distribution. To capture the characteristics of $Q$, such as moments, we can simulate a large numbers of $\boldsymbol{\varepsilon}$ and transform it to $\boldsymbol{e}$ to get the empirical estimates. For simplicity, in this study, we replaced the variance-covariance matrix of residuals $\text{cov}(\boldsymbol{e}, \boldsymbol{e}) = \boldsymbol{R}\sigma^2\boldsymbol{R}^\top = \boldsymbol{R}\sigma^2$ with a full-rank diagonal matrix $\text{diag}(\boldsymbol{R}\sigma^2)$, where $\text{diag}(.)$ sets the non-diagonal entries of a matrix to zeros. The resulting distribution for $Q$ is $N(\boldsymbol{0}_n, \text{diag}(\boldsymbol{R}\sigma^2))$.

Distribution $Q$ is derived from the correctly specified model. However, if the model is misspecified, then the actual distribution of residuals denoted as $P$, will be different from $Q$. For example, if the data generating process contains variables correlated with any column of $\boldsymbol{X}$ but missing from $\boldsymbol{X}$, causing an omitted variable problem, $P$ will be different from $Q$ because the residual operator obtained from the fitted regression model will not be the same as $\boldsymbol{R}$. Besides, if the $\boldsymbol{\varepsilon}$ follows a non-normal distribution such as a multivariate lognormal distribution, $P$ will usually be skewed and has a long tail. 

## Distance of $P$ from $Q$

Defining a proper distance between distributions is usually easier than defining a proper distance between data plots. Given the true residual distribution $Q$ and the reference residual distribution $P$, we used a distance measure based on Kullback-Leibler divergence [@kullback1951information] to quantify the difference between two distributions as

\begin{equation} \label{eq:kl-0}
D = \log\left(1 + D_{KL}\right),
\end{equation}

where $D_{KL}$ is defined as

\begin{equation} \label{eq:kl-1}
D_{KL} = \int_{\mathbb{R}^{n}}\log\frac{p(\boldsymbol{e})}{q(\boldsymbol{e})}p(\boldsymbol{e})d\boldsymbol{e},
\end{equation}

\noindent and $p(.)$ and $q(.)$ are the probability density functions for distribution $P$ and distribution $Q$, respectively.

This distance measure was first proposed in @li2024plot. It was mainly designed for measuring the effect size of non-linearity and heteroskedasticity in a residual plot. @li2024plot have derived that, for a classical normal linear regression model that omits necessary higher-order predictors $\boldsymbol{Z}$ and the corresponding parameter $\boldsymbol{\beta}_z$, and incorrectly assumes $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0}_n,\sigma^2\boldsymbol{I}_n)$ while in fact $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0}_n, \boldsymbol{V})$ where $\boldsymbol{V}$ is an arbitrary symmetric positive semi-definite matrix, $Q$ can be represented as $N(\boldsymbol{R}\boldsymbol{Z}\boldsymbol{\beta}_z, \text{diag}(\boldsymbol{R}\boldsymbol{V}\boldsymbol{R}))$. Note that the variance-covariance matrix is replaced with the diagonal matrix to ensure it is a full-rank matrix. 

Since both $P$ and $Q$ are adjusted to be multivariate normal distributions, Equation \ref{eq:kl-1} can be further expanded to


\begin{equation} \label{eq:kl-2}
D_{KL} = \frac{1}{2}\left(\log\frac{|\boldsymbol{W}|}{|\text{diag}(\boldsymbol{R}\sigma^2)|} - n + \text{tr}(\boldsymbol{W}^{-1}\text{diag}(\boldsymbol{R}\sigma^2)) + \boldsymbol{\mu}_z^\top\boldsymbol{W}^{-1}\boldsymbol{\mu}_z\right),
\end{equation}

\noindent where $\boldsymbol{\mu}_z = \boldsymbol{R}\boldsymbol{Z}\boldsymbol{\beta}_z$, and $\boldsymbol{W} = \text{diag}(\boldsymbol{R}\boldsymbol{V}\boldsymbol{R})$. The assumed error variance $\sigma^2$ is set to be $\text{tr}(\boldsymbol{V})/n$, which is the expectation of the estimated variance.


## Non-normal $P$

For non-normal error $\boldsymbol{\varepsilon}$, the true residual distribution $P$ is unlikely to be a multivariate normal distribution. Thus, Equation \ref{eq:kl-2} given in @li2024plot will not be applicable to models violating the normality assumption. 

To evaluate the Kullback-Leibler divergence of non-normal $P$ from $Q$, the fallback is to solve Equation \ref{eq:kl-1} numerically. However, since $\boldsymbol{e}$ is a linear transformation of non-normal random variables, it is very common that the general form of $P$ is unknown, meaning that we can not easily compute $p(\boldsymbol{e})$ using a well-known probability density function. Additionally, even if $p(\boldsymbol{e})$ can be calculated for any $\boldsymbol{e} \in \mathbb{R}^n$, it will be very difficult to do numerical integration over the $n$-dimensional space, because $n$ could be potentially very large.   

In order to approximate $D_{KL}$ in a practically computable manner, the elements of $\boldsymbol{e}$ are assumed to be independent of each other. This assumption solves both of the issues mentioned above. First, we no longer need to integrate over $n$ random variables. The result of Equation \ref{eq:kl-1} is now the sum of the Kullback-Leibler divergence evaluated for each individual residual due to the assumption of independence between observations. Second, it is not required to know the joint probability density $p(\boldsymbol{e})$ any more. Instead, the evaluation of Kullback-Leibler divergence for an individual residual relies on the knowledge of the marginal density $p_i(e_i)$, where $e_i$ is the $i$-th residual for $i = 1, ..., n$. This is much easier to approximate through simulation. It is also worth mentioning that this independence assumption generally will not hold if $\text{cov}(e_i, e_j) \neq 0$ for any $1 \leq i < j \leq n$, but its existence is essential for reducing the computational cost.

Given $\boldsymbol{X}$ and $\boldsymbol{\beta}$, the algorithm for approximating Equation \ref{eq:kl-1} starts from simulating $m$ sets of observed values $\boldsymbol{y}$ according to the data generating process. The observed values are stored in a matrix $\boldsymbol{A}$ with $n$ rows and $m$ columns, where each column of $\boldsymbol{A}$ is a set of observed values. Then, we can get $m$ sets of realized values of $\boldsymbol{e}$ stored in the matrix $\boldsymbol{B}$ by applying the residual operator $\boldsymbol{B} = \boldsymbol{R}\boldsymbol{A}$. Furthermore, kernel density estimation (KDE) with Gaussian kernel and optimal bandwidth selected by the Silverman's rule of thumb [@silverman2018density] is applied on each row of $\boldsymbol{B}$ to estimate $p_i(e_i)$ for $i = 1, ..., n$. The KDE computation can be done by the `density` function in R. 

Since the Kullback-Leibler divergence can be viewed as the expectation of the log-likelihood ratio between distribution $P$ and distribution $Q$ evaluated on distribution $P$, we can reuse the simulated residuals in matrix $\boldsymbol{B}$ to estimate the expectation by the sample mean. With the independence assumption, for non-normal $P$, $D_{KL}$ can be approximated by

\begin{align*} \label{eq:kl-3}
D_{KL} &\approx \sum_{i = 1}^{n} \hat{D}_{KL}^{(i)}, \\
\hat{D}_{KL}^{(i)} &= \frac{1}{m}\sum_{j = 1}^{m} \log\frac{\hat{p}_i(B_{ij})}{q(B_{ij})},
\end{align*} 

\noindent where $\hat{D}_{KL}^{(i)}$ is the estimator of the Kullback-Leibler divergence for an individual residual $e_i$, $\boldsymbol{B}_{ij}$ is the $i$-th row and $j$-th column entry of the matrix $\boldsymbol{B}$, $\hat{p}_i(.)$ is the kernel density estimator of $p_i(.)$, $q(.)$ is the normal density function with mean zero and an assumed variance estimated as $\hat{\sigma}^2 = \sum_{b \in vec(\boldsymbol{B})}(b - \sum_{b \in vec(\boldsymbol{B})} b/nm)^2/(nm - 1)$, and $vec(.)$ is the vectorization operator which turns a $n \times m$ matrix into a $nm \times 1$ column vector by stacking the columns of the matrix on top of each other.


# Distance Estimation {#sec-model-distance-estimation}

We previously defined a distance measure (Equation \ref{eq:kl-0}) to quantify the difference between the true residual distribution $P$ and an ideal reference distribution $Q$. However, this distance measure can only be computed when the data generating process is known. In reality, we often have no knowledge about the data generating process, otherwise we do not need to do a residual diagnostic in the first place.

To approximate this distance from a residual plot, we proposed a computer vision estimator $\hat{D}$ formulated as

\begin{equation} \label{eq:d-approx}
\hat{D} = f_{CV}(V_{h \times w}(\boldsymbol{e}, \hat{\boldsymbol{y}})),
\end{equation}

\noindent where $V_{h \times w}(.)$ renders the residuals vs fitted values plot as an $h \times w$ RGB image, and $f_{CV}(.)$ is a model that maps the image to an estimated distance $\hat{D} \in [0, +\infty)$.

The distance estimator $\hat{D}$ allows us to assess how closely the residuals resemble an ideal distribution and use as an index of model violation severity. However, $\hat{D}$ is not expected to equal the true distance $D$, as a single residual plot may not capture all characteristics of the residual distribution. For the same distribution $P$, simulated plots can vary visually, especially with small $n$, leading to estimation error that depends on how representative the input plot is.

# Statistical Testing {#sec-model-statistical-testing}

## Lineup Evaluation {#sec-model-lineup-evaluation}

Theoretically, the distance $D$ for a correctly specified model is $0$, as $P = Q$. However, a computer vision model may not predict $\hat{D} = 0$ for a null plot. For example, Figure \ref{fig:false-finding} shows a null plot with a pattern suggestive of heteroskedasticity. The model can not discern whether such patterns arise from heteroskedasticity or from skewed fitted values. Additionally, some null plots could have outliers or strong visual patterns due to randomness, and a reasonable model will try to summarize those information into the prediction, resulting in $\hat{D} > 0$. This is not problematic when $\hat{D}$ is large, as strong visual signals typically justify rejection of $H_0$. But when $\hat{D}$ is moderate, it is insufficient alone for decision-making.

To address this issue we can adhere to the paradigm of visual inference, by comparing the estimated distance $\hat{D}$ to the estimated distances for the null plots in a lineup. Specifically, if a lineup comprises 20 plots, $H_0$ will be rejected if $\hat{D}$ exceeds the maximum estimated distance among the $m - 1$ null plots, denoted as $\max\limits_{1 \leq i \leq m-1} {\hat{D}_{null}^{(i)}}$, where $\hat{D}_{null}^{(i)}$ represents the estimated distance for the $i$-th null plot. This approach is conceptually equivalent to the typical lineup protocol requiring a 95% significance level, where $H_0$ is rejected if the data plot is identified as the most distinct plot by the sole observer.

Moreover, if the number of plots in a lineup, denoted by $m$, is sufficiently large, the empirical distribution of ${\hat{D}_{null}^{(i)}}$ can be viewed as an approximation of the null distribution of the estimated distance. Consequently, quantiles of the null distribution can be estimated using the sample quantiles and used for decision-making purposes. The details of the sample quantile computation can be found in @hyndman1996sample. For instance, if $\hat{D}$ is greater than or equal to the 95% sample quantile, denoted as $Q_{null}(0.95)$, we can conclude that the estimated distance for the true residual plot is significantly different from the estimated distance for null plots with a 95% significance level. Based on our experience, at least 100 null plots are typically needed for stable estimation, but more may be required if the null distribution has heavy tails. Alternatively, a $p$-value is the probability of observing a distance equally or greater than $\hat{D}$ under $H_0$, and it can be estimated as $\frac{1}{m} + \frac{1}{m}\sum_{i=1}^{m-1}I\left(\hat{D}_{null}^{(i)} \geq \hat{D}\right)$.

To reduce computational cost, a pre-computed lattice of $\hat{D}$ quantiles under $H_0$ for various sample sizes can be used. By matching observed values to the closest entries in this lattice, approximate $p$-values can be obtained efficiently, with minor loss of precision.

## Bootstrapping

Bootstrap methods are commonly used in linear regression to estimate parameter variability without strong distributional assumptions [@davison1997bootstrap; @efron1994introduction]. This involves resampling observations with replacement and refitting the model.

Similarly, we can apply bootstrapping to the estimated distance $\hat{D}$. For each bootstrap sample $i = 1, \dots, n_{boot}$, we obtain a refitted model $M^{(i)}_{boot}$, a corresponding residual plot $V^{(i)}_{boot}$, and a predicted distance $\hat{D}^{(i)}_{boot}$. If we are interested in the variation of $\hat{D}$, the distribution of $\hat{D}^{(i)}_{boot}$ can be used to construct confidence intervals.

Alternatively, since each $M_{boot}^{(i)}$ has its own residual distribution, a new approximated null distribution can be construed and the corresponding 95% sample quantile $Q_{boot}^{(i)}(0.95)$ can be computed. Then, if $\hat{D}_{boot}^{(i)} \geq Q_{boot}^{(i)}(0.95)$, $H_0$ will be rejected for $M_{boot}^{(i)}$. The ratio of rejected $M_{boot}^{(i)}$ among all the refitted models reflects how often the assumed regression model are considered to be misspecified if data were repeatedly drawn from the same process. But this approach is computationally very expensive because it requires $n_{boot} \times (n_{null} + 1)$ times of residual plot assessment. In practice, $Q_{null}(0.95)$ can be used to replace $Q_{boot}^{(i)}(0.95)$ in the computation. 

# Model Violations Index {#sec-model-violations-index}

In \ref{sec-model-lineup-evaluation}, we noted that a pre-computed lattice of $\hat{D}$ quantiles can reduce the computation cost of lineup tests. Another practical approach is to assess model performance directly using the value of $\hat{D}$.

The estimator $\hat{D}$ captures the difference between the true and reference residual distributions, which reflects the extent of model violations, making it instrumental in forming a model violations index (MVI). However, when more observations are used in regression, the value of $\hat{D}$ tends to increase logarithmically. This is because $D = \log(1 + D_{KL})$, and under the assumption of independence, $D_{KL}$ is the sum of $D_{KL}^{(i)}$ across all observations. This does not mean that $\hat{D}$ becomes less reliable. In fact, larger samples often make model violations more visible in residual plots, unless strong overlapping masks the patterns.

However, to create a standardized and generalizable index, it is important to adjust for the effect of sample size. Therefore, the Model Violations Index (MVI) is proposed as

\begin{equation} \label{eq:mvi}
\text{MVI} = C + \hat{D} - \log(n),
\end{equation}

\noindent where $C$ is a sufficiently large constant to ensure the result remains positive, and the $\log(n)$ term offset the increase in $D$ with larger sample sizes.

Figure \ref{fig:poly-heter-index} displays the residual plots for fitted models exhibiting varying degrees of non-linearity and heteroskedasticity. Each residual plot's MVI is computed using Equation \ref{eq:mvi} with $C = 10$. When $\text{MVI} > 8$, the visual patterns are notably strong and easily discernible by humans. In the range $6 < \text{MVI} < 8$, the visibility of the visual pattern diminishes as MVI decreases. Conversely, when $\text{MVI} < 6$, the visual pattern tends to become relatively faint and challenging to observe. Table \ref{tab:mvi} provides a summary of the MVI usage and it is applicable to other linear regression models.


```{r}
data.frame(degree = c("Strong", "Moderate", "Weak"),
           range = c("$\\text{MVI} > 8$", "$6 < \\text{MVI} < 8$", "$\\text{MVI} < 6$")) %>%
  kableExtra::kable(format = "latex",
                    booktabs = TRUE,
                    label = "mvi",
                    caption = "Degree of model violations or the strength of the visual signals according to the Model Violations Index (MVI). The constant $C$ is set to be 10.",
                    escape = FALSE,
                    col.names = c("Degree of model violations", "Range ($C$ = 10)"),
                    align = "lc")
```


```{r}
set.seed(1495)
if (!file.exists(here::here("cached_data/poly_index.rds"))) {
  e_sigma_factor <- 3
  keras_mod <- autovi::get_keras_model("vss_phn_32")
  poly_dgp <- phn_model(j = 3, sigma = 0.1^e_sigma_factor)
  ori_dat <- poly_dgp$gen(300)
  
  vss <- c()
  actual_ss <- c()
  dat_combined <- tibble()
    
  for (e_sigma in seq(0.7, 1.8, length.out = 20)^e_sigma_factor) {
    poly_dgp <- phn_model(j = 3, sigma = e_sigma)
    dat <- poly_dgp$gen(300, computed = list(x1 = ori_dat$x1, e = ori_dat$e, k = ori_dat$k * e_sigma / 0.1^e_sigma_factor))
    mod <- lm(y ~ x1, data = dat)
    my_vi <- autovi::auto_vi(fitted_mod = mod, keras_mod = keras_mod)
    vss <- c(vss, my_vi$vss()$vss)
    actual_ss <- c(actual_ss, log(poly_dgp$sample_effect_size(dat) + 1))
    
    dat_combined <- bind_rows(dat_combined, mutate(dat, e_sigma = e_sigma))
  }
  
  saveRDS(dat_combined, here::here("cached_data/poly_index.rds"))
  saveRDS(vss, here::here("cached_data/poly_index_vss.rds"))
}

dat_combined <- readRDS(here::here("cached_data/poly_index.rds"))
vss <- readRDS(here::here("cached_data/poly_index_vss.rds"))

p1 <- dat_combined %>%
  autovi::AUTO_VI$plot_resid() +
  facet_wrap(~e_sigma, scales = "free", 
             labeller = function(...){ 
               tibble(nb = map(glue("'MVI' == {format(vss - log(300) + 10, digits = 3)}"), str2expression))
               },
             ncol = 5) +
  theme(strip.text = element_text(size = 10), plot.title = element_text(size = 10)) +
  ggtitle("(A) Non-linearity")
```

```{r}
set.seed(1495)
if (!file.exists(here::here("cached_data/heter_index.rds"))) {
  b_factor <- 2
  keras_mod <- autovi::get_keras_model("vss_phn_32")
  heter_dgp <- phn_model(include_z = FALSE, b = 0)
  ori_dat <- poly_dgp$gen(300)
  
  vss <- c()
  actual_ss <- c()
  dat_combined <- tibble()
    
  for (b in rev(seq(0.2, 4, length.out = 20))^b_factor) {
    heter_dgp <- phn_model(include_z = FALSE, b = b)
    dat <- heter_dgp$gen(300, computed = list(x1 = ori_dat$x1, e = ori_dat$e))
    mod <- lm(y ~ x1, data = dat)
    my_vi <- autovi::auto_vi(fitted_mod = mod, keras_mod = keras_mod)
    vss <- c(vss, my_vi$vss()$vss)
    actual_ss <- c(actual_ss, log(heter_dgp$sample_effect_size(dat) + 1))
    
    dat_combined <- bind_rows(dat_combined, mutate(dat, nb = -b))
  }
  
  saveRDS(dat_combined, here::here("cached_data/heter_index.rds"))
  saveRDS(vss, here::here("cached_data/heter_index_vss.rds"))
}

dat_combined <- readRDS(here::here("cached_data/heter_index.rds"))
vss <- readRDS(here::here("cached_data/heter_index_vss.rds"))

p2 <- dat_combined %>%
  autovi::AUTO_VI$plot_resid() +
  facet_wrap(~nb, scales = "free", 
             labeller = function(...){ 
               tibble(nb = map(glue("'MVI' == {format(vss - log(300) + 10, digits = 3)}"), str2expression))
               },
             ncol = 5) +
  theme(strip.text = element_text(size = 10), plot.title = element_text(size = 10)) +
  ggtitle("(B) Heteroskedasticity")
```

```{r poly-heter-index, fig.pos = "!h", fig.cap = "Residual plots generated from fitted models exhibiting varying degrees of (A) non-linearity and (B) heteroskedasticity violations. The model violations index (MVI) is displayed atop each residual plot. The non-linearity patterns are relatively strong for $MVI > 8$, and relatively weak for $MVI < 6$, while the heteroskedasticity patterns are relatively strong for $MVI > 8$, and relatively weak for $MVI < 6$.", fig.height = 10}
patchwork::wrap_plots(p1, p2, ncol = 1)
```







# Model Architecture {#sec-model-architecture}

```{r cnn-diag, fig.cap = "Diagram of the architecture of the optimized computer vision model. Numbers at the bottom of each box show the shape of the output of each layer. The band of each box drawn in a darker color indicates the use of the rectified linear unit activation function.  Yellow boxes are 2D convolutional layers, orange boxes are pooling layers, the grey box is the concatenation layer, and the purple boxes are dense layers.", out.width = "100%"}
magick::image_read_pdf("figures/cnn.pdf")
```

The architecture of the computer vision model $f_{CV}$ is adapted from the well-established VGG16 architecture [@simonyan2014very]. While more recent architectures like ResNet [@he2016deep] and DenseNet[@huang2017densely], have achieved even greater performance, VGG16 remains a solid choice for many applications due to its simplicity and effectiveness. Our decision to use VGG16 aligns with our goal of starting with a proven and straightforward model. Figure \ref{fig:cnn-diag} provides a diagram of the architecture. More details about the neural network layers used in this study are provided in the Appendix.

The model begins with an input layer of shape $n \times h \times w \times 3$, capable of handling $n$ RGB images. This is followed by a grayscale conversion layer utilizing the luma formula under the Rec. 601 standard [@series2011studio], which converts the color image to grayscale. Grayscale suffices for our task since data points are plotted in black. We experiment with three combinations of $h$ and $w$: $32 \times 32$, $64 \times 64$, and $128 \times 128$, aiming to achieve sufficiently high image resolution for the problem at hand.

The processed image is used as the input for the first convolutional block. The model comprises at most five consecutive convolutional blocks, mirroring the original VGG16 architecture. Within each block, there are two 2D convolutional layers followed by two activation layers, respectively. Subsequently, a 2D max-pooling layer follows the second activation layer. The 2D convolutional layer convolves the input with a fixed number of $3 \times 3$ convolution filters, while the 2D max-pooling layer downsamples the input along its spatial dimensions by taking the maximum value over a $2 \times 2$ window for each channel of the input. The activation layer employs the rectified linear unit (ReLU) activation function, a standard practice in deep learning, which introduces a non-linear transformation of the output of the 2D convolutional layer. Additionally, to regularize training, a batch normalization layer is added after each 2D convolutional layer and before the activation layer. Finally, a dropout layer is appended at the end of each convolutional block to randomly set some inputs to zero during training, further aiding in regularization.

The output of the last convolutional block is summarized by either a global max-pooling layer or a global average-pooling layer, resulting in a two-dimensional tensor. To enrich predictions with additional information beyond visual features, this tensor is concatenated with an additional $n \times 5$ tensor, which contains the "Monotonic", "Sparse", "Splines", and "Striped" measures computed using the `cassowaryr` R package [@mason2022cassowaryr] , along with the number of observations for $n$ residual plots. These measures were selected for their reliability and efficiency, as other scagnostics occasionally caused R process crashes ($\sim 5\%$) during data preparation due to a bug in the `interp` package [@Albrecht2023interp]. Although this bug was later fixed at our request, the fix came too late to re-train the model. Moreover, their high computational cost makes them unsuitable for fast inference.

The concatenated tensor is then fed into the final prediction block. This block consists of two fully-connected layers. The first layer contains at least $128$ units, followed by a dropout layer. A batch normalization layer is inserted between the fully-connected layer and the dropout layer for regularization purposes. The second fully-connected layer consists of only one unit, serving as the output of the model.

The model weights $\boldsymbol{\theta}$ were randomly initialized using the Glorot Uniform method [@glorot2010understanding] and they were optimized by the Adam optimizer [@kingma2014adam] with the mean square error loss function

$$\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta}}{\text{arg min}}\frac{1}{n_{\text{train}}}\sum_{i=1}^{n_{\text{train}}}(D_i - f_{\boldsymbol{\theta}}(V_i, S_i))^2,$$

\noindent where $n_{\text{train}}$ is the number of training samples, $V_i$ is the $i$-th residual plot and $S_i$ is the auxiliary information about the $i$-th residual plot including four scagnostics and the number of observations. 

# Data Generation and Model Training {#sec-model-data-generation}

To enable supervised training of computer vision models for detecting violations in linear regression, we generated a synthetic dataset of 80,000 training and 8,000 test residual plots. This simulation-based approach provided full control over the data-generating process, allowing precise computation of $D$, cost-effective data scaling, and the ability to model diverse visual patterns of model violations.

The simulated data incorporated three common types of residual departures: non-linearity, heteroskedasticity, and non-normality. These were introduced by fitting a standard simple linear regression model to data generated from a more flexible model based on Hermite polynomials [@hermite1864nouveau], multiple predictors, and varying distributions in both predictors and error terms. A comprehensive grid of parameter combinations was explored to generate a wide range of residual patterns. Importantly, the simulation included scenarios with multiple violations occurring simultaneously. To ensure uniform coverage across the difficulty scale of the target variable $D$, a bucket sampling scheme was used to create a balanced dataset. Samples were iteratively simulated and accepted into one of 50 buckets, each representing a distinct range of $D$.

Model training was conducted on the MASSIVE M3 high-performance computing platform [@goscinski2014multi] using TensorFlow [@abadi2016tensorflow] and Keras [@chollet2015keras]. Hyperparameters were optimized via Bayesian tuning with KerasTuner [@omalley2019kerastuner], minimizing validation RMSE across 100 trials. The tuning process included early stopping and considered dropout rate, batch normalization, input resolution, and auxiliary inputs.

Further details, including mathematical formulations of the simulation model, parameter specifications, sampling scheme, example residual plots and hyperparameter tuning configuration, are provided in the Appendix.


# Results {#sec-model-results}

## Model Performance

The test performance for the optimized models with three different input sizes are summarized in Table \ref{tab:performance}. Among these models, the $32 \times 32$ model consistently exhibited the best test performance. The mean absolute error of the $32 \times 32$ model indicated that the difference between $\hat{D}$ and $D$ was approximately $0.43$ on the test set, a negligible deviation considering the normal range of $D$ typically falls between $0$ and $7$. The high $R^2$ values also suggested that the predictions were largely linearly correlated with the target.

Figure \ref{fig:model-performance} presents a hexagonal heatmap for $D - \hat{D}$ versus $D$. The brown smoothing curves, fitted by generalized additive models [@hastie2017generalized], demonstrate that all the optimized models perform admirably on the test sets when $1.5 < D < 6$, where no structural issues are noticeable. However, over-predictions occurred when $D < 1.5$, while under-predictions occurred predominantly when $\hat{D} > 6$.  

For input images representing null plots where $D = 0$, it was expected that the models will over-predict the distance, as explained in Section \ref{sec-model-lineup-evaluation}. However, it can not explain the under-prediction issue. Therefore, we analysed the relationship between residuals and all the factors involved in the data generating process. We found that most issues actually arose from non-linearity problems and the presence of a second predictor in the regression model as illustrated in Figure \ref{fig:over-under}. When the variance for the error distribution was small, the optimized model tended to under-predict the distance. Conversely, when the error distribution had a large variance, the model tended to over-predict the distance.

Since most of the deviation stemmed from the presence of non-linearity violations, to further investigate this, we split the test set based on violation types and re-evaluated the performance, as detailed in Table \ref{tab:performance-sub}. It was evident that metrics for null plots were notably worse compared to other categories. Furthermore, residual plots solely exhibiting non-normality issues were the easiest to predict, with very low test root mean square error (RMSE) at around $0.3$. Residual plots with non-linearity issues were more challenging to assess than those with heteroskedasticity or non-normality issues. When multiple violations were introduced to a residual plot, the performance metrics typically lay between the metrics for each individual violation.

Based on the model performance metrics, we chose to use the best-performing model evaluated on the test set, namely the $32 \times 32$ model, for the subsequent analysis.



```{r}
test_pred <- readr::read_csv(here::here("data/test_pred.csv"))
train_pred <- readr::read_csv(here::here("data/train_pred.csv"))
meta <- readr::read_csv(here::here("data/meta.csv"))

test_summary <- test_pred %>% 
  left_join(meta) %>%
  group_by(res) %>%
  summarise(RMSE = yardstick::rmse_vec(effect_size, vss),
            R2 = yardstick::rsq_vec(effect_size, vss),
            MAE = yardstick::mae_vec(effect_size, vss),
            HUBER = yardstick::huber_loss_vec(effect_size, vss)) %>%
  mutate(type = "test")

train_summary <- train_pred %>% 
  left_join(meta) %>%
  group_by(res) %>%
  summarise(RMSE = yardstick::rmse_vec(effect_size, vss),
            R2 = yardstick::rsq_vec(effect_size, vss),
            MAE = yardstick::mae_vec(effect_size, vss),
            HUBER = yardstick::huber_loss_vec(effect_size, vss)) %>%
  mutate(type = "train")
```

```{r}
test_summary %>%
  select(res, RMSE, R2, MAE, HUBER) %>%
  mutate(res = glue::glue("${res} \\times {res}$")) %>%
  mutate(across(RMSE:MAE, ~format(.x, digits = 3))) %>%
  mutate(across(HUBER, ~format(.x, digits = 2))) %>%
  kableExtra::kable(format = "latex",
                    booktabs = TRUE, 
                    label = "performance",
                    caption = "The test performance of three optimized models with different input sizes.",
                    escape = FALSE,
                    align = "lrrrr",
                    col.names = c("", "RMSE", "$R^2$", "MAE", "Huber loss"))
```

```{r model-performance, fig.pos="!h", fig.cap = "Hexagonal heatmap for difference in $D$ and $\\hat{D}$ vs $D$ on test data for three optimized models with different input sizes. The brown lines are smoothing curves produced by fitting generalized additive models. The area over the zero line in light yellow indicates under-prediction, and the area under the zero line in light green indicates over-prediction.", dev = 'png', dpi = 300, fig.height = 3}
model_pred <- train_pred %>% 
  left_join(meta) %>%
  mutate(type = "train") %>%
  bind_rows(test_pred %>% 
              left_join(meta) %>%
              mutate(type = "test"))

model_pred %>%
  filter(type == "test") %>%
  ggplot() +
  geom_hline(yintercept = 0, alpha = 0.5) +
  annotate("rect", 
           ymin = min(model_pred$effect_size - model_pred$vss),
           ymax = 0,
           xmin = min(model_pred$effect_size),
           xmax = max(model_pred$effect_size),
            fill = "#40B0A6",
            alpha = 0.3) +
  geom_point(data = NULL, aes(5, 0, col = "over-prediction"), shape = 15) +
  annotate("rect", 
           ymin = 0,
           ymax = max(model_pred$effect_size - model_pred$vss),
           xmin = min(model_pred$effect_size),
           xmax = max(model_pred$effect_size),
           fill = "#E1BE6A",
           alpha = 0.3) +
  geom_point(data = NULL, aes(5, 0, col = "under-prediction"), shape = 15) +
  geom_hex(aes(effect_size, effect_size - vss), bins = 20) +
  geom_smooth(aes(effect_size, effect_size - vss), se = FALSE, col = "#994F00") +
  facet_wrap(~ res) +
  coord_fixed() +
  ylab(expression(D - hat(D))) +
  xlab(expression(D)) +
  theme_light() +
  scale_fill_continuous(limits = c(1, 6000), trans = "log10", low = "#56B1F7", high = "#132B43") +
  scale_color_manual(values = c("under-prediction" = scales::alpha("#E1BE6A", 0.3),
                                "over-prediction" = scales::alpha("#40B0A6", 0.3)),
                     breaks = c("under-prediction", "over-prediction")) +
  guides(col = guide_legend(title = NULL, override.aes = list(size = 8), order = 1))
```


```{r over-under, fig.pos="!h", fig.cap = "Scatter plots for difference in $D$ and $\\hat{D}$ vs $\\sigma$ on test data for the $32 \\times 32$ optimized model. The data is grouped by whether the regression has only non-linearity violation, and whether it includes a second predictor in the regression formula. The brown lines are smoothing curves produced by fitting generalized additive models. The area over the zero line in light yellow indicates under-prediction, and the area under the zero line in light green indicates over-prediction.", dev = 'png', dpi = 300}
model_pred %>%
  filter(res == 32L) %>%
  filter(type == "test") %>%
  mutate(only_z = include_z & !include_heter & !include_non_normal) %>%
  rename(`Only has non-linearity violation` = only_z, `Has second predictor` = include_x2) %>%
  ggplot() +
  geom_hline(yintercept = 0, alpha = 0.5) +
  annotate("rect", 
           ymin = min(model_pred$effect_size[model_pred$type == "test"] - model_pred$vss[model_pred$type == "test"]),
                ymax = 0,
                xmin = min(model_pred$e_sigma),
                xmax = max(model_pred$e_sigma),
            fill = "#40B0A6",
            alpha = 0.3) +
  annotate("rect", 
           ymin = 0,
                ymax = max(model_pred$effect_size[model_pred$type == "test"] - model_pred$vss[model_pred$type == "test"]),
                xmin = min(model_pred$e_sigma),
                xmax = max(model_pred$e_sigma),
            fill = "#E1BE6A",
            alpha = 0.3) +
  geom_point(aes(e_sigma, effect_size - vss), alpha = 0.2) +
  geom_smooth(aes(e_sigma, effect_size - vss), se = FALSE, col = "#994F00") +
  facet_grid(`Has second predictor` ~ `Only has non-linearity violation`,
             labeller = label_both) +
  theme_light() +
  ylab(expression(D - hat(D))) +
  xlab(expression(sigma))
```



```{r}
model_pred %>%
  filter(res == 32L) %>%
  group_by(include_non_normal, include_heter, include_z) %>%
  summarise(train_n = sum(type == "train"),
            train_RMSE = yardstick::rmse_vec(effect_size[type == "train"], vss[type == "train"]),
            test_n = sum(type == "test"),
            test_RMSE = yardstick::rmse_vec(effect_size[type == "test"], vss[type == "test"])) %>%
  mutate(violations = ifelse(include_z, "non-linearity", "null")) %>%
  mutate(violations = ifelse(include_heter, glue("{violations} + heteroskedasticity"), violations)) %>%
  mutate(violations = ifelse(include_non_normal, glue("{violations} + non-normality"), violations)) %>%
  mutate(violations = gsub("null \\+ ", "", violations)) %>%
  ungroup() %>%
  select(violations, train_n, train_RMSE, test_n, test_RMSE) %>%
  mutate(train_n = format(train_n)) %>%
  mutate(test_n = format(test_n)) %>%
  mutate(across(c(train_RMSE, test_RMSE), ~format(.x, digits = 3))) %>%
  mutate(violations = ifelse(violations == "null", "no violations", violations)) %>%
  select(-train_n, -train_RMSE) %>%
  kableExtra::kable(format = "latex",
                    booktabs = TRUE,
                    label = "performance-sub",
                    caption = "The training and test performance of the $32 \\times 32$ model presented with different model violations.",
                    escape = FALSE,
                    linesep = "", 
                    align = "lrr",
                    col.names = c("Violations", "\\#samples", "RMSE"))
```


<!-- - check if j has an impact on residuals more carefully -->
<!-- - the RMSE has very little to do with j if x2 is not included. And its impact is not as large as sigma. -->


## Comparison with Human Visual Inference and Conventional Tests

### Overview of the Human Subject Experiment

In order to check the validity of the proposed computer vision model, residual plots presented in the human subject experiment conducted by @li2024plot will be assessed.


<!-- The experiment revealed that conventional tests are more sensitive to weak departures from model assumptions than visual test, and they often reject the null hypothesis when departures are not visibly different from null residual plots. -->

This study has collected 7,974 human responses to 1,152 lineups. Each lineup contains one randomly placed true residual plot and 19 null plots. Among the 1,152 lineups, 24 are attention check lineups in which the visual patterns are designed to be extremely obvious and very different from the corresponding to null plots, 36 are null lineups where all the lineups consist of only null plots, 279 are lineups with uniform predictor distribution evaluated by 11 participants, and the remaining 813 are lineups with discrete, skewed or normal predictor distribution evaluated by 5 participants. Attention check lineups and null lineups will not be assessed in the following analysis. 

In @li2024plot, the residual plots are simulated from a data generating process that corresponds to a special case of the synthetic data model used in this study. A key feature of the design is that model violations are introduced independently, meaning non-linearity and heteroskedasticity do not co-exist within a single lineup but are instead assigned uniformly across different lineups. Moreover, the experimental design does not account for non-normality or multiple predictors.


### Model Performance on the Human-evaluated Data

```{r rd-human}
vss_32 <- readRDS(here::here("data/vss_32.rds"))

experiment <- vi_survey %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  select(unique_lineup_id, attention_check, null_lineup, prop_detect,
         answer, effect_size, conventional_p_value,
         p_value, type, shape, a, b, x_dist, e_dist,
         e_sigma, include_z, k, n)

experiment <- vss_32 %>%
  left_join(experiment) %>%
  mutate(conventional_reject = conventional_p_value <= 0.05) %>%
  mutate(reject = p_value <= 0.05) %>%
  mutate(model_reject = vss_p_value <= 0.05) %>%
  mutate(model_boot_reject = vss_boot_p_value <= 0.05)

experiment <- experiment %>%
  left_join(readRDS(here::here("data/actual_ss.rds")))
```


```{r}
experiment %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  group_by(type) %>%
  summarise(RMSE = yardstick::rmse_vec(actual_ss, vss),
            R2 = yardstick::rsq_vec(actual_ss, vss),
            MAE = yardstick::mae_vec(actual_ss, vss),
            HUBER = yardstick::huber_loss_vec(actual_ss, vss)) %>%
  mutate(across(RMSE:HUBER, ~format(.x, digits = 3))) %>%
  kableExtra::kable(format = "latex",
                    booktabs = TRUE, 
                    label = "experiment-performance",
                    caption = "The performance of the $32 \\times 32$ model on the data used in the human subject experiment.",
                    escape = FALSE,
                    align = "lrrrr",
                    col.names = c("Violation", "RMSE", "$R^2$", "MAE", "Huber loss"))
```


For each lineup used in @li2024plot, there is one true residual plot and 19 null plots. While the distance $D$ for the true residual plot depends on the underlying data generating process, the distance $D$ for the null plots is zero. We have used our optimized computer vision model to estimate distance for both the true residual plots and the null plots. To have a fair comparison, $H_0$ will be rejected if the true residual plot has the greatest estimated distance among all plots in a lineup. Additionally, the appropriate conventional tests including the Ramsey Regression Equation Specification Error Test (RESET) [@ramsey1969tests] for non-linearity and the Breusch-Pagan test  [@breusch1979simple] for heteroskedasticity were applied on the same data for comparison.

The performance metrics of $\hat{D}$ for true residual plots are outlined in Table \ref{tab:experiment-performance}. It is notable that all performance metrics are slightly worse than those evaluated on the test data. Nevertheless, the mean absolute error remains at a low level, and the linear correlation between the prediction and the true value remains very high. Consistent with results in Table \ref{tab:performance-sub}, lineups with non-linearity issues are more challenging to predict than those with heteroskedasticity issues.


Table \ref{tab:human-conv-table} provides a summary of the agreement between decisions made by the computer vision model and conventional tests. The agreement rates between conventional tests and the computer vision model are 85.95% and 79.69% for residual plots containing heteroskedasticity and non-linearity patterns, respectively. These figures are higher than those calculated for visual tests conducted by human, indicating that the computer vision model exhibits behavior more akin to the best available conventional tests. However, Figure \ref{fig:conv-mosaic} shows that the computer vision model does not always reject when the conventional tests reject. And a small number of plots will be rejected by computer vision model but not by conventional tests.  This suggests that conventional tests are more sensitive than the computer vision model.

Figure \ref{fig:pcp} further illustrates the decisions made by visual tests conducted by human, computer vision models, and conventional tests, using a parallel coordinate plots. It can be observed that all three tests will agree with each other for around 50% of the cases. When visual tests conducted by human do not reject, there are substantial amount of cases where computer vision model also do not reject but conventional tests reject. There are much fewer cases that do not reject by visual tests and conventional tests, but is rejected by computer vision models. This indicates computer vision model can behave like visual tests conducted by human better than conventional tests. Moreover, there are great proportion of cases where visual tests conducted by human is the only test who does not reject. 


When plotting the decision against the distance, as illustrated in Figure \ref{fig:power}, several notable observations emerge. Firstly, compared to conventional tests, the computer vision model tends to have fewer rejected cases when $D < 2$ and fewer non-rejected cases when $2< D < 4$. This suggests tests based on the computer vision model are less sensitive to small deviations from model assumptions than conventional tests but more sensitive to moderate deviations. Additionally, visual tests demonstrate the lowest sensitivity to residual plots with small distances where not many residual plots are rejected when $D < 2$. Similarly, for large distances where $D > 4$, almost all residual plots are rejected by the computer vision model and conventional tests, but for visual tests conducted by humans, the threshold is higher with $D > 5$.

In Figure \ref{fig:power}, rejection decisions are fitted by logistic regression models with no intercept terms and an offset equals to $\text{log}(0.05/0.95)$. The fitted curves for the computer vision model fall between those of conventional tests and visual tests for both non-linearity and heteroskedasticity, which means there is still potential to refine the computer vision model to better align its behavior with visual tests conducted by humans.

In the experiment conducted in @li2024plot, participants were allowed to make multiple selections for a lineup. The weighted detection rate was computed by assigning weights to each detection. If the participant selected zero plots, a weight of 0.05 was assigned; otherwise, if the true residual plot was detected, the weight was 1 divided by the number of selections. This weighted detection rate allow us to assess the quality of the distance measure purposed in this paper, by using the $\delta$-difference statistic. The $\delta$-difference is originally defined by @chowdhury2018measuring, is given by

$$
\delta = \bar{d}_{\text{true}} - \underset{j}{\text{max}}\left(\bar{d}_{\text{null}}^{(j)}\right) \quad \text{for}~j = 1,...,m-1,
$$


where $\bar{d}_{\text{null}}^{(j)}$ is the mean distance between the $j$-th null plot and the other null plots, $\bar{d}_{\text{true}}$ is the mean distance between the true residual plot and null plots, and $m$ is the number of plots in a lineup. These mean distances are used because, as noted by @chowdhury2018measuring, the distances can vary depending on which data plot is used for comparison. For instance, with three null plots, A, B and C, the distance between A and B may differ from the distance between A and C. To obtain a consistent distance for null plot A, averaging is necessary. However, this approach is not applicable to the distance proposed in this paper, as we only compare the residual plot against a theoretically good residual plot. Consequently, the statistic must be adjusted to evaluate our distance measure effectively.

One important aspect that the $\delta$-difference was designed to capture is the empirical distribution of distances for null plot. If we were to replace the mean distances $\bar{d}_{\text{null}}^{(j)}$ directly with $D_{\text{null}}^{(j)}$, the distance of the $j$-th null plot, the resulting distribution would be degenerate, since $D_{null}$ equals zero by definition. Additionally, $D$ can not be derived from an image, meaning it falls outside the scope of the distances considered by @chowdhury2018measuring. Instead, the focus should be on the empirical distribution of $\hat{D}$, as it influences decision-making. Therefore, the adjusted $\delta$-different is defined as

$$
\delta_{\text{adj}} = \hat{D} - \underset{j}{\text{max}}\left(\hat{D}_{\text{null}}^{(j)}\right) \quad \text{for}~j = 1,...,m-1,
$$


\noindent where $\hat{D}_{\text{null}}^{(j)}$ is the estimated distance for the $j$-th null plot, and $m$ is the number of plots in a lineup.

Figure \ref{fig:delta} displays the scatter plot of the weighted detection rate vs the adjusted $\delta$-difference. It indicates that the weighted detection rate increases as the adjusted $\delta$-difference increases, particularly when the adjusted $\delta$-difference is greater than zero. A negative adjusted $\delta$-difference suggests that there is at least one null plot in the lineup with a stronger visual signal than the true residual plot. In some instances, the weighted detection rate is close to one, yet the adjusted $\delta$-difference is negative. This discrepancy implies that the distance measure, or the estimated distance, may not perfectly reflect actual human behavior.

```{r hist-null-human}
lineup_vss <- readRDS(here::here("data/lineup_vss.rds"))
lineup_vss <- lineup_vss %>%
  group_by(unique_lineup_id) %>%
  mutate(delta_diff = vss[!null] - max(vss[null])) %>%
  mutate(gamma_diff = sum(vss[null] > vss[!null]))


experiment <- experiment %>%
  left_join(lineup_vss %>%
  filter(null == FALSE) %>%
  mutate(model_reject_20 = rank == 1) %>%
  select(unique_lineup_id, rank, model_reject_20))
```

```{r cache = TRUE}
vi_lineup <- get_vi_lineup()
```

```{r}
bind_rows(
  experiment %>%
    filter(!attention_check) %>%
    filter(!null_lineup) %>%
    mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
    group_by(type) %>%
    summarise(n = format(n()), agree = format(sum(model_reject == conventional_reject)), rate = format(mean(conventional_reject == model_reject), digits = 4)),
  experiment %>%
    filter(!attention_check) %>%
    filter(!null_lineup) %>%
    mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
    group_by(type) %>%
    summarise(n = format(n()), agree = format(sum(model_reject == reject)), rate = format(mean(model_reject == reject), digits = 4))
  ) %>%
  kableExtra::kable(format = "latex",
                    booktabs = TRUE,
                    label = "human-conv-table",
                    caption = "Summary of the comparison of decisions made by computer vision model with decisions made by conventional tests and visual tests conducted by human.",
                    escape = FALSE,
                    align = "lrrr",
                    col.names = c("Violations", "\\#Samples", "\\#Agreements", "Agreement rate")) %>%
  kableExtra::pack_rows("Compared with conventional tests", 1, 2) %>%
  kableExtra::pack_rows("Compared with visual tests conducted by human", 3, 4)
```


```{r conv-mosaic, fig.pos = "!h", fig.cap = "Rejection rate ($p$-value $\\leq0.05$) of computer vision models conditional on conventional tests on non-linearity (left) and heteroskedasticity (right) lineups displayed using a mosaic plot. When the conventional test fails to reject, the computer vision mostly fails to reject the same plot as well as indicated by the height of the top right yellow rectangle, but there are non negliable amount of plots where the conventional test rejects but the computer vision model fails to reject as indicated by the width of the top left yellow rectangle."}
library(ggmosaic)

experiment %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  mutate(type = ifelse(type == "non-linearity", "Non-linearity", "Heteroskedasticity")) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity"))) %>%
  mutate(model_reject_20 = ifelse(model_reject_20, "Reject", "No")) %>%
  mutate(conventional_reject = ifelse(conventional_reject, "Reject", "No")) %>%
  mutate(across(c(model_reject_20, conventional_reject), ~factor(.x, levels = c("Reject", "No")))) %>%
  ggplot() +
  geom_mosaic(aes(x = ggmosaic::product(model_reject_20, conventional_reject), 
                  fill = model_reject_20)) +
  facet_grid(~type) +
  ylab("Computer vision model rejects") +
  xlab("Conventional tests reject ") +
  # labs(fill = "Visual tests reject") +
  scale_fill_manual(values = c("#40B0A6", "#E1BE6A")) + 
  theme_bw() +
  theme(legend.position = "none") +
  coord_fixed()
```


```{r power, fig.pos = "!h", fig.cap = "Comparison of power of visual tests, conventional tests and the computer vision model. Marks along the x-axis at the bottom of the plot represent rejections made by each type of test. Marks at the top of the plot represent acceptances. Power curves are fitted by logistic regression models with no intercept but an offset equals to $\\text{log}(0.05/0.95)$."}

max_actual_ss <- max(filter(experiment, !attention_check, !null_lineup)$actual_ss)
max_poly_ss <- max(filter(experiment, !attention_check, !null_lineup, type == "polynomial")$actual_ss)
max_heter_ss <- max(filter(experiment, !attention_check, !null_lineup, type != "polynomial")$actual_ss)

model_glm_pred_poly <- glm(model_reject_20 ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type == "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_poly_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_poly_ss, 0.01), type = "Non-linearity", name = "Computer vision model", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

visual_glm_pred_poly <- glm(reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type == "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_poly_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_poly_ss, 0.01), type = "Non-linearity", name = "Visual test", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

conv_glm_pred_poly <- glm(conventional_reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type == "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_poly_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_poly_ss, 0.01), type = "Non-linearity", name = "Conventional test", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

model_glm_pred_heter <- glm(model_reject_20 ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type != "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_heter_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_heter_ss, 0.01), type = "Heteroskedasticity", name = "Computer vision model", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

visual_glm_pred_heter <- glm(reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type != "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_heter_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_heter_ss, 0.01), type = "Heteroskedasticity", name = "Visual test", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

conv_glm_pred_heter <- glm(conventional_reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type != "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_heter_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_heter_ss, 0.01), type = "Heteroskedasticity", name = "Conventional test", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

mutate(experiment, type = ifelse(type == "polynomial", "Non-linearity", "Heteroskedasticity")) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity"))) %>%
  filter(!null_lineup, !attention_check) %>%
ggplot() +
  geom_line(data = model_glm_pred_poly, aes(d, value, col = "Computer vision model"), linewidth = 0.8) +
  geom_line(data = model_glm_pred_heter, aes(d, value, col = "Computer vision model"), linewidth = 0.8) +
  geom_line(data = visual_glm_pred_poly, aes(d, value, col = "Visual test"), linewidth = 0.8) +
  geom_line(data = visual_glm_pred_heter, aes(d, value, col = "Visual test"), linewidth = 0.8) +
  geom_line(data = conv_glm_pred_poly, aes(d, value, col = "Conventional test"), linewidth = 0.8) +
  geom_line(data = conv_glm_pred_heter, aes(d, value, col = "Conventional test"), linewidth = 0.8) +
  geom_segment(data = ~filter(.x, !reject), aes(x = actual_ss, xend = actual_ss, y = -0.1, yend = -0.1 + 0.03, col = "Visual test"), alpha = 0.4) +
  geom_segment(data = ~filter(.x, reject), aes(x = actual_ss, xend = actual_ss, y = 1.1 - 0.03 * 2, yend = 1.1 - 0.03 * 3, col = "Visual test"), alpha = 0.4) +
  geom_segment(data = ~filter(.x, !model_reject_20), aes(x = actual_ss, xend = actual_ss, y = -0.1 + 0.03, yend = -0.1 + 0.03 * 2, col = "Computer vision model"), alpha = 0.4) +
  geom_segment(data = ~filter(.x, model_reject_20), aes(x = actual_ss, xend = actual_ss, y = 1.1 - 0.03, yend = 1.1 - 0.03 * 2, col = "Computer vision model"), alpha = 0.4) +
  geom_segment(data = ~filter(.x, !conventional_reject), aes(x = actual_ss, xend = actual_ss, y = -0.1 + 0.03 * 2, yend = -0.1 + 0.03 * 3, col = "Conventional test"), alpha = 0.4) +
  geom_segment(data = ~filter(.x, conventional_reject), aes(x = actual_ss, xend = actual_ss, y = 1.1, yend = 1.1 - 0.03, col = "Conventional test"), alpha = 0.4) +
  facet_grid(~type, scales = "free_x") +
  theme_light() +
  scale_color_brewer("", palette = "Dark2") +
  theme(legend.position = "bottom") +
  xlab("D") +
  ylab("Reject")
```

```{r pcp, fig.cap = "Parallel coordinate plots of decisions made by computer vision model, conventional tests and visual tests made by human.", eval = TRUE, fig.height = 4}
library(ggpcp)
mutate(experiment, type = ifelse(type == "polynomial", "Non-linearity", "Heteroskedasticity")) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity"))) %>%
  filter(!null_lineup, !attention_check) %>%
  select(type, actual_ss, model_reject_20, reject, conventional_reject) %>%
  mutate(test = rnorm(n())) %>%
  mutate(across(c(model_reject_20, conventional_reject, reject), ~ifelse(.x, "Reject", "Not reject"))) %>%
  mutate(agree = model_reject_20 == reject) %>%
  pcp_select(c(reject, model_reject_20, conventional_reject)) %>%
  group_by(type) %>%
  pcp_scale() %>%
  ungroup() %>%
  pcp_arrange(method = "from-right") %>%
  ggplot() +
  geom_pcp(aes_pcp(), linewidth = 0.1) +
  geom_pcp_labels(aes_pcp()) +
  geom_pcp_boxes(aes_pcp(), boxwidth = 0.1) +
  facet_wrap(~type, scales = "free_y") +
  theme_light() +
  scale_x_discrete(labels = c("Visual\ntest", "Computer\nvision\nmodel", "Conventional\ntest")) +
  labs(col = "D") +
  ylab("") +
  xlab("") +
  theme(axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title = element_blank())
```


```{r delta, fig.pos = "!h", fig.cap = "A weighted detection rate vs adjusted $\\delta$-difference plot. The brown line is smoothing curve produced by fitting generalized additive models.", fig.height = 4}
experiment %>%
  left_join(lineup_vss %>%
              group_by(unique_lineup_id) %>%
              summarise(delta_diff = first(delta_diff), gamma_diff = first(gamma_diff))) %>%
  ggplot() +
  geom_vline(xintercept = 0, alpha = 0.4, linetype = 2) +
  geom_point(aes(delta_diff, prop_detect), alpha = 0.3) +
  geom_smooth(aes(delta_diff, prop_detect), se = FALSE, col = "#994F00") +
  ylab("Weighted detection rate") +
  xlab(expression(delta[adj])) +
  theme_light()
```



# Examples {#sec-examples}


In this section, we present the performance of trained computer vision model on three example datasets. These include the dataset associated with the residual plot displaying a "left-triangle" shape, as displayed in Figure \ref{fig:false-finding}, along with the Boston housing dataset [@harrison1978hedonic], and the "dino" datasets from the `datasauRus` R package [@datasaurus]. 

The first example illustrates a scenario where both the computer vision model and human visual inspection successfully avoid rejecting $H_0$ when $H_0$ is true, contrary to conventional tests. This underscores the necessity of visually examining the residual plot.

In the second example, we encounter a more pronounced violation of the model, resulting in rejection of $H_0$ by all three tests. This highlights the practicality of the computer vision model, particularly for less intricate tasks.

The third example presents a situation where the model deviation is non-typical. Here, the computer vision model and human visual inspection reject $H_0$, whereas some commonly used conventional tests do not. This emphasizes the benefits of visual inspection and the unique advantage of the computer vision model, which, like humans, makes decisions based on visual discoveries.

## Left-triangle

In Section \ref{sec-model-introduction}, we presented an example residual plot showcased in Figure \ref{fig:false-finding}, illustrating how humans might misinterpret the "left-triangle" shape as indicative of heteroskedasticity. Additionally, the Breusch-Pagan test yielded a rejection with a $p$-value of 0.046, despite the residuals originating from a correctly specified model. Figure \ref{fig:false-lineup} offers a lineup for this fitted model, showcasing various degrees of "left-triangle" shape across all residual plots. This phenomenon is evidently caused by the skewed distribution of the fitted values. Notably, if the residual plot were evaluated through a visual test, it would not be rejected since the true residual plot positioned at 10 can not be distinguished from the others.

Figure \ref{fig:false-check} presents the results of the assessment by the computer vision model. Notably, the observed visual signal strength is considerably lower than the 95% sample quantile of the null distribution. Moreover, the bootstrapped distribution suggests that it is highly improbable for the fitted model to be misspecified as the majority of bootstrapped fitted models will not be rejected. Thus, for this particular fitted model, both the visual test and the computer vision model will not reject $H_0$. However, the Breusch-Pagan test will reject $H_0$ because it can not effectively utilize information from null plots.

The attention map at Figure \ref{fig:false-check}B suggests that the estimation is highly influenced by the top-right and bottom-right part of the residual plot, as it forms two vertices of the triangular shape. A principal component analysis (PCA) is also performed on the output of the global pooling layer of the computer vision model. As mentioned in @simonyan2014very, a computer vision model built upon the convolutional blocks can be viewed as a feature extractor. For the $32 \times 32$ model, there are 256 features outputted from the global pooling layer, which can be further used for different visual tasks not limited to distance prediction. To see if these features can be effectively used for distinguishing null plots and true residual plot, we linearly project them into the first and second principal components space as shown in Figure \ref{fig:false-check}D. It can be observed that because the bootstrapped plots are mostly similar to the null plots, the points drawn in different colors are mixed together. The true residual plot is also covered by both the cluster of null plots and cluster of bootstrapped plots. This accurately reflects our understanding of Figure \ref{fig:false-lineup}. 

```{r}
set.seed(452)
ori_x <- rand_lognormal()
dat <- heter_model(b = 0, x = closed_form(~-ori_x))$gen(300)

mod <- lm(y ~ x, data = dat)
my_vi <- autovi::auto_vi(fitted_model = mod)
```

```{r}
if (!file.exists(here::here("cached_data/fig1_check.rds"))) {
  keras_model <- autovi::get_keras_model("vss_phn_32")
  my_vi$check(null_draws = 200L, boot_draws = 200L, keras_model = keras_model, extract_feature_from_layer = "global_max_pooling2d")
  saveRDS(my_vi$check_result, here::here("cached_data/fig1_check.rds"))
}

```

```{r}
my_vi$check_result <- readRDS(here::here("cached_data/fig1_check.rds"))
```

```{r}
define_gradient <- function() {
  reticulate::py_run_string(glue::glue(
"
import tensorflow as tf
import PIL
import numpy as np

def get_smooth_gradient(keras_mod, plot_path, target_size, input_auxiliary = None, noise_level = 0.1, n = 50):
    im = PIL.Image.open(plot_path)
    im = im.resize(target_size)
    input_im = tf.keras.utils.img_to_array(im)
    input_im = np.reshape(input_im, tuple([1]) + tuple(target_size) + tuple([3]))
    if input_auxiliary is not None:
        input_auxiliary = np.reshape(np.array(input_auxiliary), (1, 5))
    
    if input_auxiliary is not None:
        input_auxiliary = tf.Variable(input_auxiliary)
    
    im_grad = None
    sigma = noise_level * (np.max(input_im) - np.min(input_im))
    
    for i in range(n):    
        new_input_im = input_im + np.random.normal(0, sigma, input_im.shape)
        new_input_im = tf.Variable(new_input_im)
        
        with tf.GradientTape() as tape:
            if input_auxiliary is not None:
                pred = keras_mod([new_input_im, input_auxiliary])
            else:
                pred = keras_mod(new_input_im)
        new_im_grad = tape.gradient(pred, new_input_im)
        new_im_grad = tf.image.rgb_to_grayscale(new_im_grad)[0].numpy()
        if im_grad is None:
            im_grad = new_im_grad
        else:
            im_grad = im_grad + new_im_grad
    
    im_grad = im_grad / n
    
    input_im = tf.Variable(input_im)
    if input_auxiliary is not None:
        with tf.GradientTape() as tape:
            pred = keras_mod([input_im, input_auxiliary])
        auxiliary_grad = tape.gradient(pred, input_auxiliary)
    if input_auxiliary is not None:
        return (im_grad, auxiliary_grad[0].numpy())
    else:
        return im_grad
        

def get_raw_gradient(keras_mod, plot_path, target_size, input_auxiliary = None):
    im = PIL.Image.open(plot_path)
    im = im.resize(target_size)
    input_im = tf.keras.utils.img_to_array(im)
    input_im = np.reshape(input_im, tuple([1]) + tuple(target_size) + tuple([3]))
    if input_auxiliary is not None:
        input_auxiliary = np.reshape(np.array(input_auxiliary), (1, 5))
    
    input_im = tf.Variable(input_im)
    if input_auxiliary is not None:
        input_auxiliary = tf.Variable(input_auxiliary)
    
    with tf.GradientTape() as tape:
        if input_auxiliary is not None:
            pred = keras_mod([input_im, input_auxiliary])
        else:
            pred = keras_mod(input_im)
    im_grad = tape.gradient(pred, input_im)
    
    if input_auxiliary is not None:
        with tf.GradientTape() as tape:
            pred = keras_mod([input_im, input_auxiliary])
        auxiliary_grad = tape.gradient(pred, input_auxiliary)
    
    im_grad = tf.image.rgb_to_grayscale(im_grad)
    if input_auxiliary is not None:
        return (im_grad[0].numpy(), auxiliary_grad[0].numpy())
    else:
        return im_grad[0].numpy()
"
))
}

```



```{r}
if (!file.exists(here::here("cached_data/fig1_attention_map.rds"))) {
  reticulate::py_set_seed(10086, disable_hash_randomization = TRUE)
  define_gradient()
  keras_mod <- autovi::get_keras_model("vss_phn_32")
  plot_path <- my_vi$plot_resid() %>%
    autovi::save_plot()
  input_auxiliary <- my_vi$auxiliary()

  reticulate::py$get_smooth_gradient(keras_mod = keras_mod, 
                                     plot_path = plot_path, 
                                     target_size = c(32L, 32L), 
                                     input_auxiliary = input_auxiliary,
                                     noise_level = 0.5,
                                     n = 1000L) -> smooth_g
  
  x_range <- ggplot2::ggplot_build(my_vi$plot_resid())$layout$panel_params[[1]]$x.range
  y_range <- ggplot2::ggplot_build(my_vi$plot_resid())$layout$panel_params[[1]]$y.range
  
  smooth_att_map <- smooth_g[[1]] |> 
    as.data.frame() |>
    dplyr::mutate(row = rev(1:32)) |>
    tidyr::pivot_longer(V1:V32, names_to = "column", values_to = "gradient") |>
    dplyr::mutate(column = as.integer(gsub("V", "", column))) |>
    dplyr::mutate(column = x_range[1] + column * (x_range[2] - x_range[1])/(32 - 1)) |>
    dplyr::mutate(row = y_range[1] + row * (y_range[2] - y_range[1])/(32 - 1))
  
  scale_zero_one <- function(x) (x - min(x))/(max(x) - min(x))
  
  p <- ggplot() +
    ggplot2::geom_raster(data = mutate(smooth_att_map, gradient = scale_zero_one(gradient)), ggplot2::aes(column, row, fill = gradient), alpha = 0.7) +
    scale_fill_gradient(low = "black", high = "white") +
    ggplot2::theme_void() +
    theme(legend.position = "none")
  
  saveRDS(p, here::here("cached_data/fig1_attention_map.rds"))
  
}
```


```{r false-check, fig.pos = "!h", fig.cap = 'A summary of the residual plot assessment evaluted on 200 null plots and 200 bootstrapped plots. (A) The true residual plot exhibiting a "left-triangle" shape. (B) The attention map produced by computing the gradient of the output with respect to the greyscale input.  (C) The density plot of estimated distance for null plots and bootstrapped plots. The green area indicates the distribution of estimated distances for bootstrapped plots, while the yellow area represents the distribution of estimated distances for null plots. The fitted model will not be rejected since $\\hat{D} < Q_{null}(0.95)$. (D) plot of first two principal components of features extracted from the global pooling layer of the computer vision model.  ', fig.height = 8}
p1 <- my_vi$plot_resid(theme = theme_light()) + ggtitle(glue::glue("(A) Residual plot (MVI = {format(my_vi$check_result$observed$vss - log(nrow(my_vi$get_fitted_and_resid())) + 10, digits = 3)}, weak)")) 
p2 <- readRDS(here::here("cached_data/fig1_attention_map.rds")) + ggtitle("(B) Attention map")
p3 <- my_vi$summary_plot() +
  ggtitle("(C) Density plot of estimated D", subtitle = glue::glue("Conventional p-value = {format(HETER_MODEL$test(dat)$p_value, digits = 3)}")) +
  theme(legend.position = "bottom", legend.box = "vertical") +
  labs(linetype = "") +
  xlab(expression(hat(D))) +
  scale_linetype(labels = c(expression(Q[null](0.95)), expression(observed~hat(D)))) +
  scale_fill_manual(values = c("#40B0A6", "#E1BE6A")) +
  scale_color_manual(values = c("#40B0A6", "#E1BE6A"))
pca_result <- my_vi$feature_pca()
pc1_per <- (attributes(pca_result)$sdev^2/256)[1]
pc2_per <- (attributes(pca_result)$sdev^2/256)[2]
p4 <- pca_result %>%
  mutate(set = ifelse(set == "boot", "Boot", set)) %>%
  mutate(set = ifelse(set == "null", "Null", set)) %>%
  ggplot() +
  geom_point(data = ~filter(.x, set != "observed"), aes(PC1, PC2, col = set), size = 2, alpha = 0.3) +
  geom_point(data = ~filter(.x, set == "observed"), aes(PC1, PC2), size = 4, col = "black", fill = "black") +
  geom_text(data = ~filter(.x, set == "observed"), aes(PC1, PC2, label = "Observed"), nudge_x = 10) +
  theme_light() +
  scale_color_brewer(palette = "Dark2") +
  xlab(glue::glue("PC1 ({scales::percent(pc1_per)})")) +
  ylab(glue::glue("PC2 ({scales::percent(pc2_per)})")) +
  labs(col = "") +
  theme(legend.position = "bottom") +
  ggtitle("(D) PCA for extracted features") +
  scale_color_manual(values = c("#40B0A6", "#E1BE6A"))

patchwork::wrap_plots(p1, p2, p3, p4, nrow = 2, widths = c(1, 1), heights = c(1, 1))
```

```{r false-lineup, fig.pos = "!h", fig.cap = 'A lineup of residual plots displaying "left-triangle" visual patterns. The true residual plot occupies position 10, yet there are no discernible visual patterns that distinguish it from the other plots.', fig.height = 8}
pos <- 10
result <- tibble()
for (i in c(1:20)[-pos]) {
  result <- bind_rows(result,
                      my_vi$rotate_resid() %>%
                        mutate(k = i))
}
result <- bind_rows(my_vi$get_fitted_and_resid() %>% 
                      mutate(k = pos),
                    result)
result %>%
  VI_MODEL$plot_lineup(remove_grid_line = TRUE,
                       remove_axis = TRUE,
                       theme = theme_light())
```

## Boston Housing

The Boston housing dataset, originally published by @harrison1978hedonic, offers insights into housing in the Boston, Massachusetts area. For illustration purposes, we utilize a reduced version from Kaggle, comprising 489 rows and 4 columns: average number of rooms per dwelling (RM), percentage of lower status of the population (LSTAT), pupil-teacher ratio by town (PTRATIO), and median value of owner-occupied homes in $1000's (MEDV). In our analysis, MEDV will serve as the response variable, while the other columns will function as predictors in a linear regression model. Our primary focus is to detect non-linearity, because the relationships between RM and MEDV or LSTAT and MEDV are non-linear.

Figure \ref{fig:boston-check} displays the residual plot and the assessment conducted by the computer vision model. A clear non-linearity pattern resembling a "U" shape is shown in the plot A. Furthermore, the RESET test yields a very small $p$-value. The estimated distance $\hat{D}$ significantly exceeds $Q_{null}(0.95)$, leading to rejection of $H_0$. The bootstrapped distribution also suggests that almost all the bootstrapped fitted models will be rejected, indicating that the fitted model is unlikely to be correctly specified. The attention map in plot B suggests the center of the image has higher leverage than other areas, and it is the turning point of the "U" shape. The PCA provided in plot D shows two distinct clusters of data points, further underling the visual differences between bootstrapped plots and null plots. This coincides the findings from Figure \ref{fig:boston-lineup}, where the true plot exhibiting a "U" shape is visually distinctive from null plots. If a visual test is conducted by human, $H_0$ will also be rejected.


```{r}
housing <- read_csv(here::here("data/housing.csv"))
mod <- lm(MEDV ~ ., data = housing)

my_vi <- autovi::auto_vi(fitted_model = mod)
```

```{r}
if (!file.exists(here::here("cached_data/boston_check.rds"))) {
  keras_model <- autovi::get_keras_model("vss_phn_32")
  my_vi$check(null_draws = 200L, boot_draws = 200L, keras_model = keras_model, extract_feature_from_layer = "global_max_pooling2d")
  saveRDS(my_vi$check_result, here::here("cached_data/boston_check.rds"))
}

```

```{r}
my_vi$check_result <- readRDS(here::here("cached_data/boston_check.rds"))
```


```{r}
if (!file.exists(here::here("cached_data/boston_attention_map.rds"))) {
  reticulate::py_set_seed(10086, disable_hash_randomization = TRUE)
  define_gradient()
  keras_mod <- autovi::get_keras_model("vss_phn_32")
  plot_path <- my_vi$plot_resid() %>%
    autovi::save_plot()
  input_auxiliary <- my_vi$auxiliary()

  reticulate::py$get_smooth_gradient(keras_mod = keras_mod, 
                                     plot_path = plot_path, 
                                     target_size = c(32L, 32L), 
                                     input_auxiliary = input_auxiliary,
                                     noise_level = 0.5,
                                     n = 1000L) -> smooth_g
  
  x_range <- ggplot2::ggplot_build(my_vi$plot_resid())$layout$panel_params[[1]]$x.range
  y_range <- ggplot2::ggplot_build(my_vi$plot_resid())$layout$panel_params[[1]]$y.range
  
  smooth_att_map <- smooth_g[[1]] |> 
    as.data.frame() |>
    dplyr::mutate(row = rev(1:32)) |>
    tidyr::pivot_longer(V1:V32, names_to = "column", values_to = "gradient") |>
    dplyr::mutate(column = as.integer(gsub("V", "", column))) |>
    dplyr::mutate(column = x_range[1] + column * (x_range[2] - x_range[1])/(32 - 1)) |>
    dplyr::mutate(row = y_range[1] + row * (y_range[2] - y_range[1])/(32 - 1))
  
  scale_zero_one <- function(x) (x - min(x))/(max(x) - min(x))
  
  p <- ggplot() +
    ggplot2::geom_raster(data = mutate(smooth_att_map, gradient = scale_zero_one(gradient)), ggplot2::aes(column, row, fill = gradient), alpha = 0.7) +
    scale_fill_gradient(low = "black", high = "white") +
    ggplot2::theme_void() +
    theme(legend.position = "none")
  
  saveRDS(p, here::here("cached_data/boston_attention_map.rds"))
  
}
```


```{r boston-check, fig.pos = "!h", fig.cap = 'A summary of the residual plot assessment for the Boston housing fitted model evaluted on 200 null plots and 200 bootstrapped plots. (A) The true residual plot exhibiting a "U" shape. (B) The attention map produced by computing the gradient of the output with respect to the greyscale input.  (C) The density plot of estimated distance for null plots and bootstrapped plots. The blue area indicates the distribution of estimated distances for bootstrapped plots, while the yellow area represents the distribution of estimated distances for null plots. The fitted model will be rejected since $\\hat{D} \\geq Q_{null}(0.95)$. (D) plot of first two principal components of features extracted from the global pooling layer of the computer vision model. ', fig.height = 8}
p1 <- my_vi$plot_resid(theme = theme_light()) + ggtitle(glue::glue("(A) Residual plot (MVI = {format(my_vi$check_result$observed$vss - log(nrow(my_vi$get_fitted_and_resid())) + 10, digits = 3)}, strong)")) 
p2 <- readRDS(here::here("cached_data/boston_attention_map.rds")) + ggtitle("(B) Attention map")
p3 <- my_vi$summary_plot() +
  ggtitle("(C) Density plot of estimated D", subtitle = glue::glue("Conventional p-value = {format(lmtest::resettest(mod)$p.value, digits = 3)}")) +
  theme(legend.position = "bottom", legend.box = "vertical") +
  labs(linetype = "") +
  xlab(expression(hat(D))) +
  scale_linetype(labels = c(expression(Q[null](0.95)), expression(observed~hat(D)))) +
  scale_fill_manual(values = c("#40B0A6", "#E1BE6A")) +
  scale_color_manual(values = c("#40B0A6", "#E1BE6A"))
pca_result <- my_vi$feature_pca()
pc1_per <- (attributes(pca_result)$sdev^2/256)[1]
pc2_per <- (attributes(pca_result)$sdev^2/256)[2]
p4 <- pca_result %>%
  mutate(set = ifelse(set == "boot", "Boot", set)) %>%
  mutate(set = ifelse(set == "null", "Null", set)) %>%
  ggplot() +
  geom_point(data = ~filter(.x, set != "observed"), aes(PC1, PC2, col = set), size = 2, alpha = 0.3) +
  geom_point(data = ~filter(.x, set == "observed"), aes(PC1, PC2), size = 4, col = "black", fill = "black") +
  geom_text(data = ~filter(.x, set == "observed"), aes(PC1, PC2, label = "Observed"), nudge_x = 7) +
  theme_light() +
  scale_color_brewer(palette = "Dark2") +
  xlab(glue::glue("PC1 ({scales::percent(pc1_per)})")) +
  ylab(glue::glue("PC2 ({scales::percent(pc2_per)})")) +
  labs(col = "") +
  theme(legend.position = "bottom") +
  ggtitle("(D) PCA for extracted features") +
  scale_color_manual(values = c("#40B0A6", "#E1BE6A"))

patchwork::wrap_plots(p1, p2, p3, p4, nrow = 2, widths = c(1, 1), heights = c(1, 1))
```

```{r boston-lineup, fig.pos = "!h", fig.cap = 'A lineup of residual plots for the Boston housing fitted model. The true residual plot is at position 7. It can be easily identified as the most different plot.', fig.height = 8}
pos <- 7
result <- tibble()
for (i in c(1:20)[-pos]) {
  result <- bind_rows(result,
                      my_vi$rotate_resid() %>%
                        mutate(k = i))
}
result <- bind_rows(my_vi$get_fitted_and_resid() %>% 
                      mutate(k = pos),
                    result)
result %>%
  VI_MODEL$plot_lineup(remove_grid_line = TRUE,
                       remove_axis = TRUE,
                       theme = theme_light())
```




## DatasauRus

The computer vision model possesses the capability to detect not only typical issues like non-linearity, heteroskedasticity, and non-normality but also artifact visual patterns resembling real-world objects, as long as they do not appear in null plots. These visual patterns can be challenging to categorize in terms of model violations. Therefore, we will employ the RESET test, the Breusch-Pagan test, and the Shapiro-Wilk test [@shapiro1965analysis] for comparison.

The "dino" dataset within the `datasauRus` R package exemplifies this scenario. With only two columns, x and y, fitting a regression model to this data yields a residual plot resembling a "dinosaur", as displayed in Figure \ref{fig:dino-check}A. Unsurprisingly, this distinct residual plot stands out in a lineup, as shown in Figure \ref{fig:dino-lineup}. A visual test conducted by humans would undoubtedly reject $H_0$.

According to the residual plot assessment by the computer vision model, $\hat{D}$ exceeds $Q_{null}(0.95)$, warranting a rejection of $H_0$. Additionally, most of the bootstrapped fitted models will be rejected, indicating an misspecified model. However, both the RESET test and the Breusch-Pagan test yield $p$-values greater than 0.3, leading to a non-rejection of $H_0$. Only the Shapiro-Wilk test rejects the normality assumption with a small $p$-value. 

More importantly, the attention map in Figure \ref{fig:dino-check}B clearly exhibits a "dinosaur" shape, strongly suggesting that the distance prediction is based on human-perceptible visual patterns. The computer vision model effectively captures the contour or outline of the embedded shape, similar to how humans interpret residual plots. Additionally, the PCA in Figure \ref{fig:dino-check}D demonstrates that the cluster of bootstrapped plots is positioned at the corner of the cluster of null plots.

In practice, without accessing the residual plot, it would be challenging to identify the artificial pattern of the residuals. Moreover, conducting a normality test for a fitted regression model is not always standard practice among analysts. Even when performed, violating the normality assumption is sometimes deemed acceptable, especially considering the application of quasi-maximum likelihood estimation in linear regression. This example underscores the importance of evaluating residual plots and highlights how the proposed computer vision model can facilitate this process.


```{r}
dino <- datasauRus::datasaurus_dozen %>% filter(dataset == "dino")
mod <- lm(y ~ ., data = select(dino, -dataset))

my_vi <- autovi::auto_vi(fitted_model = mod)
```

```{r}
if (!file.exists(here::here("cached_data/dino_check.rds"))) {
  keras_model <- autovi::get_keras_model("vss_phn_32")
  my_vi$check(null_draws = 200L, boot_draws = 200L, keras_model = keras_model, extract_feature_from_layer = "global_max_pooling2d")
  saveRDS(my_vi$check_result, here::here("cached_data/dino_check.rds"))
}
```

```{r}
my_vi$check_result <- readRDS(here::here("cached_data/dino_check.rds"))
```

```{r}
if (!file.exists(here::here("cached_data/dino_attention_map.rds"))) {
  reticulate::py_set_seed(10086, disable_hash_randomization = TRUE)
  define_gradient()
  keras_mod <- autovi::get_keras_model("vss_phn_32")
  plot_path <- my_vi$plot_resid() %>%
    autovi::save_plot()
  input_auxiliary <- my_vi$auxiliary()

  reticulate::py$get_smooth_gradient(keras_mod = keras_mod, 
                                     plot_path = plot_path, 
                                     target_size = c(32L, 32L), 
                                     input_auxiliary = input_auxiliary,
                                     noise_level = 0.5,
                                     n = 1000L) -> smooth_g
  
  x_range <- ggplot2::ggplot_build(my_vi$plot_resid())$layout$panel_params[[1]]$x.range
  y_range <- ggplot2::ggplot_build(my_vi$plot_resid())$layout$panel_params[[1]]$y.range
  
  smooth_att_map <- smooth_g[[1]] |> 
    as.data.frame() |>
    dplyr::mutate(row = rev(1:32)) |>
    tidyr::pivot_longer(V1:V32, names_to = "column", values_to = "gradient") |>
    dplyr::mutate(column = as.integer(gsub("V", "", column))) |>
    dplyr::mutate(column = x_range[1] + column * (x_range[2] - x_range[1])/(32 - 1)) |>
    dplyr::mutate(row = y_range[1] + row * (y_range[2] - y_range[1])/(32 - 1))
  
  scale_zero_one <- function(x) (x - min(x))/(max(x) - min(x))
  
  p <- ggplot() +
    ggplot2::geom_raster(data = mutate(smooth_att_map, gradient = scale_zero_one(gradient)), ggplot2::aes(column, row, fill = gradient), alpha = 0.7) +
    scale_fill_gradient(low = "black", high = "white") +
    ggplot2::theme_void() +
    theme(legend.position = "none")
  
  saveRDS(p, here::here("cached_data/dino_attention_map.rds"))
  
}
```

```{r dino-check, fig.pos = "!h", fig.cap = 'A summary of the residual plot assessment for the datasauRus fitted model evaluated on 200 null plots and 200 bootstrapped plots. (A) The residual plot exhibits a "dinosaur" shape. (B) The attention map produced by computing the gradient of the output with respect to the greyscale input.  (C) The density plot of estimated distance for null plots and bootstrapped plots. The blue area indicates the distribution of estimated distances for bootstrapped plots, while the yellow area represents the distribution of estimated distances for null plots. The fitted model will be rejected since $\\hat{D} \\geq Q_{null}(0.95)$. (D) plot of first two principal components of features extracted from the global pooling layer of the computer vision model.', fig.height = 8}

p1 <- my_vi$plot_resid(theme = theme_light()) + ggtitle(glue::glue("(A) Residual plot (MVI = {format(my_vi$check_result$observed$vss - log(nrow(my_vi$get_fitted_and_resid())) + 10, digits = 3)}, strong)")) 
p2 <- readRDS(here::here("cached_data/dino_attention_map.rds")) + ggtitle("(B) Attention map")
p3 <- my_vi$summary_plot() +
  ggtitle("(C) Density plot of estimated D", subtitle = glue::glue("RESET test p-value = {format(lmtest::resettest(mod)$p.value, digits = 3)}\n Breusch-Pagan test p-value = {format(lmtest::bptest(mod)$p.value, digits = 3)} \n Shapiro-Wilk test p-value = {format(shapiro.test(mod$residuals)$p.value, digits = 3)}")) +
  theme(legend.position = "bottom", legend.box = "vertical") +
  labs(linetype = "") +
  xlab(expression(hat(D))) +
  scale_linetype(labels = c(expression(Q[null](0.95)), expression(observed~hat(D)))) +
  scale_fill_manual(values = c("#40B0A6", "#E1BE6A")) +
  scale_color_manual(values = c("#40B0A6", "#E1BE6A"))
pca_result <- my_vi$feature_pca()
pc1_per <- (attributes(pca_result)$sdev^2/256)[1]
pc2_per <- (attributes(pca_result)$sdev^2/256)[2]
p4 <- pca_result %>%
  mutate(set = ifelse(set == "boot", "Boot", set)) %>%
  mutate(set = ifelse(set == "null", "Null", set)) %>%
  ggplot() +
  geom_point(data = ~filter(.x, set != "observed"), aes(PC1, PC2, col = set), size = 2, alpha = 0.3) +
  geom_point(data = ~filter(.x, set == "observed"), aes(PC1, PC2), size = 4, col = "black", fill = "black") +
  geom_text(data = ~filter(.x, set == "observed"), aes(PC1, PC2, label = "Observed"), nudge_x = -10) +
  theme_light() +
  scale_color_brewer(palette = "Dark2") +
  xlab(glue::glue("PC1 ({scales::percent(pc1_per)})")) +
  ylab(glue::glue("PC2 ({scales::percent(pc2_per)})")) +
  labs(col = "", shape = "") +
  theme(legend.position = "bottom", legend.box="vertical") +
  ggtitle("(D) PCA for extracted features") +
  scale_color_manual(values = c("#40B0A6", "#E1BE6A"))

patchwork::wrap_plots(p1, p2, p3, p4, nrow = 2, widths = c(1, 1), heights = c(1, 1))
```

```{r dino-lineup, fig.pos = "!h", fig.cap = 'A lineup of residual plots for the fitted model on the "dinosaur" dataset. The true residual plot is at position 17. It can be easily identified as the most different plot as the visual pattern is extremely artificial.', fig.height = 8}
pos <- 17
result <- tibble()
for (i in c(1:20)[-pos]) {
  result <- bind_rows(result,
                      my_vi$rotate_resid() %>%
                        mutate(k = i))
}
result <- bind_rows(my_vi$get_fitted_and_resid() %>% 
                      mutate(k = pos),
                    result)
result %>%
  VI_MODEL$plot_lineup(remove_grid_line = TRUE,
                       remove_axis = TRUE,
                       theme = theme_light())
```


# Limitations and Future Work

Despite the computer vision model performing well with general cases under the synthetic data generation scheme and the three examples used in this paper, this study has several limitations that could guide future work.

The proposed distance measure assumes that the true model is a classical normal linear regression model, which can be restrictive. Although this paper does not address the relaxation of this assumption, there are potential methods to evaluate other types of regression models. The most comprehensive approach would be to define a distance measure for each different class of regression model and then train the computer vision model following the methodology described in this paper. To accelerate training, one could use the convolutional blocks of our trained model as a feature extractor and perform transfer learning on top of it, as these blocks effectively capture shapes in residual plots. Another approach would be to transform the residuals so they are roughly normally distributed and have constant variance. If only raw residuals are used, the distance-based statistical testing compares the difference in distance to a classical normal linear regression model for the true plot and null plots. This comparison is meaningful only if the difference can be identified by the distance measure proposed in this paper.

There are other types of residual plots commonly used in diagnostics, such as residuals vs. predictor and quantile-quantile plots. In this study, we focused on the most commonly used residual plot as a starting point for exploring the new field of automated visual inference. Similarly, we did not explore other, more sophisticated computer vision model architectures and specifications for the same reason. While the performance of the computer vision model is acceptable, there is still room for improvement to achieve behavior more closely resembling that of humans interpreting residual plots. This may require external survey data or human subject experiment data to understand the fundamental differences between our implementation and human evaluation.


# Conclusions

In this paper, we have introduced a distance measure based on Kullback-Leibler divergence to quantify the disparity between the residual distribution of a fitted classical normal linear regression model and the reference residual distribution assumed under correct model specification. This distance measure effectively captures the magnitude of model violations in misspecified models. We propose a computer vision model to estimate this distance, utilizing the residual plot of the fitted model as input. The resulting estimated distance serves as the foundation for constructing a single Model Violations Index (MVI), facilitating the quantification of various model violations.

Moreover, the estimated distance enables the development of a formal statistical testing procedure by evaluating a large number of null plots generated from the fitted model. Additionally, employing bootstrapping techniques and refitting the regression model allows us to ascertain how frequently the fitted model is considered misspecified if data were repeatedly obtained from the same data generating process.

The trained computer vision model demonstrates strong performance on both the training and test sets, although it exhibits slightly lower performance on residual plots with non-linearity visual patterns compared to other types of violations. The statistical tests relying on the estimated distance predicted by the computer vision model exhibit lower sensitivity compared to conventional tests but higher sensitivity compared to visual tests conducted by humans. While the estimated distance generally mirrors the strength of the visual signal perceived by humans, there remains scope for further improvement in its performance.

Several examples are provided to showcase the effectiveness of the proposed method across different scenarios, emphasizing the similarity between visual tests and distance-based tests. Overall, both visual tests and distance-based tests can be viewed as ensemble of tests, aiming to assess 
any violations of model assumptions collectively. In contrast, individual residual diagnostic tests such as the RESET test and the Breusch-Pagan test only evaluate specific violations of model assumptions. In practice, selecting an appropriate set of statistical tests for regression diagnostics can be challenging, particularly given the necessity of adjusting the significance level for each test. 

Our method holds significant value as it helps alleviate a portion of analysts' workload associated with assessing residual plots. While we recommend analysts to continue reading residual plots whenever feasible, as they offer invaluable insights, our approach serves as a valuable tool for automating the diagnostic process or for supplementary purposes when needed.

# Acknowledgement {-}

These `R` packages were used for the work: `tidyverse` [@tidyverse], `lmtest` [@lmtest], `mpoly` [@mpoly], `ggmosaic` [@ggmosaic], `kableExtra` [@kableextra], `patchwork` [@patchwork], `rcartocolor` [@rcartocolor], `glue` [@glue], `ggpcp` [@ggpcp], `here` [@here], `magick` [@magick], `yardstick` [@yardstick] and `reticulate` [@reticulate]. 

The article was created with R packages `rticles` [@rticles], `knitr` [@knitr] and `rmarkdown` [@rmarkdown]. The project's GitHub repository (https://github.com/TengMCing/auto_residual_reading_paper) contains all materials required to reproduce this article.
