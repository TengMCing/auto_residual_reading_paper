---
title: Appendix to "Automated Assessment of Residual Plots with Computer Vision Models"
author:
  - name: Weihao Li
    affil: a, b
    email: patrick.li@anu.edu.au
  - name: Dianne Cook
    affil: a
    email: dicook@monash.edu
  - name: Emi Tanaka
    affil: a, b, c
    email: emi.tanaka@anu.edu.au
  - name: Susan VanderPlas
    affil: d
    email: susan.vanderplas@unl.edu
  - name: Klaus Ackermann
    affil: a
    email: klaus.ackermann@monash.edu 
affiliation:
  - num: a
    address: |
      Department of Econometrics and Business Statistics, Monash University, Clayton, VIC, Australia
  - num: b
    address: |
      Biological Data Science Institute, Australian National University, Acton, ACT, Australia
  - num: c
    address: |
      Research School of Finance, Actuarial Studies and Statistics, Australian National University, Acton, ACT, Australia
  - num: d
    address: |
      Department of Statistics, University of Nebraska, Lincoln, Nebraska, USA
bibliography: bibliography.bib
header-includes: |
  \usepackage{lscape}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage{setspace}
  \doublespacing
output: rticles::tf_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "100%", 
  fig.align = "center")
```

```{r}
# Visual inference models and p-value calculation
# remotes::install_github("TengMCing/visage")
options(tinytex.clean = FALSE)
library(tidyverse)
library(visage)
library(glue)

# To control the simulation in this file
set.seed(10086)
```

\appendix

# Data Generation {#sec-model-data-generation}

## Simulation Scheme

While observational data is frequently employed in training models for real-world applications, the data generating process of observational data often remains unknown, making computation for our target variable $D$ unattainable. Consequently, the computer vision models developed in this study were trained using synthetic data, including 80,000 training images and 8,000 test images. This approach provided us with precise label annotations. Additionally, it ensured a large and diverse training dataset, as we had control over the data generating process, and the simulation of the training data was relatively cost-effective.

We have incorporated three types of residual departures of linear regression model in the training data, including non-linearity, heteroskedasticity and non-normality. All three departures can be summarized by the data generating process formulated as


\begin{align} \label{eq:data-sim}
\boldsymbol{y} &= \boldsymbol{1}_n + \boldsymbol{x}_1 + \beta_1\boldsymbol{x}_2 + \beta_2(\boldsymbol{z} + \beta_1\boldsymbol{w}) + \boldsymbol{k} \odot \boldsymbol{\varepsilon}, \\
\boldsymbol{z} &= \text{He}_j(g(\boldsymbol{x}_1, 2)), \\
\boldsymbol{w} &= \text{He}_j(g(\boldsymbol{x}_2, 2)), \\
\boldsymbol{k} &= \left[\boldsymbol{1}_n + b(2 - |a|)(\boldsymbol{x}_1 + \beta_1\boldsymbol{x}_2 - a\boldsymbol{1}_n)^{\circ2}\right]^{\circ1/2},
\end{align}


\noindent where $\boldsymbol{y}$, $\boldsymbol{x}_1$, $\boldsymbol{x}_2$, $\boldsymbol{z}$, $\boldsymbol{w}$, $\boldsymbol{k}$ and $\boldsymbol{\varepsilon}$ are vectors of size $n$, $\boldsymbol{1}_n$ is a vector of ones of size $n$, $\boldsymbol{x}_1$ and $\boldsymbol{x}_2$ are two independent predictors, $\text{He}_j(.)$ is the $j$th-order probabilist's Hermite polynomials [@hermite1864nouveau], $(.)^{\circ2}$ and $(.)^{\circ1/2}$ are Hadamard square and square root, $\odot$ is the Hadamard product, and $g(\boldsymbol{x}, k)$ is a scaling function to enforce the support of the random vector to be $[-k, k]^n$ defined as

$$g(\boldsymbol{x}, k) = 2k \cdot \frac{\boldsymbol{x} - x_{\min}\boldsymbol{1}_n}{x_{\max} - x_{\min}} - k\boldsymbol{1}_n,~for~k > 0,$$
\noindent where $x_{\min} = \underset{i \in \{ 1,...,n\}}{\min} x_i$, $x_{\max} = \underset{i \in \{ 1,...,n\}}{\max} x_i$ and $x_i$ is the $i$-th entry of $\boldsymbol{x}$.


```{r}
tibble(Factor = c("j", "a", "b", "$\\beta_1$", "$\\beta_2$", " $\\text{dist}_\\varepsilon$", "$\\text{dist}_{x1}$", "$\\text{dist}_{x2}$", "$\\sigma_{\\varepsilon}$", "$\\sigma_{X1}$", "$\\sigma_{X2}$", "n"),
       Domain = c("\\{2, 3, ..., 18\\}", "[-1, 1]", "[0, 100]", "\\{0, 1\\}", "\\{0, 1\\}", "\\{discrete, uniform, normal, lognormal\\}", "\\{discrete, uniform, normal, lognormal\\}", "\\{discrete, uniform, normal, lognormal\\}", "[0.0625, 9]", "[0.3, 0.6]", "[0.3, 0.6]", "[50, 500]")) %>%
  kableExtra::kable(format = "latex",
                    escape = FALSE,
                    label = "factor",
                    caption = "Factors used in the data generating process for synthetic data simulation. Factor $j$ and $a$ controls the non-linearity shape and the heteroskedasticity shape respectively. Factor $b$, $\\sigma_\\varepsilon$ and $n$ control the signal strength. Factor $\\text{dist}_\\varepsilon$, $\\text{dist}_{x1}$ and $\\text{dist}_{x2}$ specifies the distribution of $\\varepsilon$, $X_1$ and $X_2$ respectively.",
                    booktabs = TRUE)
```

The residuals and fitted values of the fitted model were obtained by regressing $\boldsymbol{y}$ on $\boldsymbol{x}_1$. If $\beta_1 \neq 0$, $\boldsymbol{x}_2$ was also included in the design matrix. This data generation process was adapted from @li2024plot, where it was utilized to simulate residual plots exhibiting non-linearity and heteroskedasticity visual patterns for human subject experiments. A summary of the factors utilized in Equation \ref{eq:data-sim} is provided in Table \ref{tab:factor}.

In Equation \ref{eq:data-sim}, $\boldsymbol{z}$ and $\boldsymbol{w}$ represent higher-order terms of $\boldsymbol{x}_1$ and $\boldsymbol{x}_2$, respectively. If $\beta_2 \neq 0$, the regression model will encounter non-linearity issues. Parameter $j$ serves as a shape parameter that controls the number of tuning points in the non-linear pattern. Typically, higher values of $j$ lead to an increase in the number of tuning points, as illustrated in Figure \ref{fig:different-j}.

```{r different-j, fig.pos="!h", fig.cap = "Non-linearity forms generated for the synthetic data simulation. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$."}

set.seed(10086)

# Data for shape 1
dat_shape_1 <- phn_model(j = 2, include_x2 = FALSE, sigma = 0.05)$gen(500) %>%
  mutate(j = 2)

# Generate data for shape 2, 3 and 4. Reuse x and e.
map_df(3:18, function(j) {
  phn_model(j = j, include_x2 = FALSE, sigma = 0.05)$
    gen(500, computed = select(dat_shape_1, x1, e)) %>%
  mutate(j = j)
}) %>%
  
  # Combined with data for shape 1
  bind_rows(dat_shape_1) %>%
  mutate(j = factor(j)) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~j, scales = "free", labeller = label_parsed, ncol = 5)
```

Additionally, scaling factor $\boldsymbol{k}$ directly affects the error distribution and it is correlated with $\boldsymbol{x}_1$ and $\boldsymbol{x}_2$. If $b \neq 0$ and $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0}_n, \sigma^2\boldsymbol{I}_n)$, the constant variance assumption will be violated. Parameter $a$ is a shape parameter controlling the location of the smallest variance in a residual plot as shown in Figure \ref{fig:different-a}.

```{r different-a, fig.width=8/5*3*2, fig.height=6/4*3*2, fig.pos="!h", fig.cap = 'Heteroskedasticity forms generated for the synthetic data simulation. Different shapes are controlled by the continuous factor $a$ between -1 and 1. For $a = -1$, the residual plot exhibits a "left-triangle" shape. And for $a = 1$, the residual plot exhibits a "right-triangle" shape. '}

set.seed(10085)

# Generate data for a = -1
dat_a_n1 <- phn_model(include_z = FALSE,
                      include_x2 = FALSE,
                      a = -1,
                      b = 100)$gen(500) %>%
  mutate(a = -1)

# Generate data for other a
map(c(-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1), function(a) {
  phn_model(include_z = FALSE,
            include_x2 = FALSE,
            a = a,
            b = 100)$gen(500) %>%
  mutate(a = a)
}) %>%
  
  # Combined with data for a = -1
  bind_rows(dat_a_n1) %>%
  mutate(a = factor(a)) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~a, scales = "free", ncol = 3) +
  xlab("Fitted values") +
  ylab("Residuals")
```

```{r different-e, fig.pos="!h", fig.cap = 'Non-normality forms generated for the synthetic data simulation. Four different error distributions including discrete, lognormal, normal and uniform are considered.'}

set.seed(10086)

# Data for shape 1
dat_shape_1 <- phn_model(include_z = FALSE, include_x2 = FALSE, e = rand_uniform(-1.4, 1.4))$gen(500) %>%
  mutate(e_dist = "uniform")

dat_shape_2 <- phn_model(include_z = FALSE, include_x2 = FALSE, sigma = 0.8)$gen(500) %>%
  mutate(e_dist = "normal")

dat_shape_3 <- phn_model(include_z = FALSE, include_x2 = FALSE, e = rand_lognormal(sigma = 0.6))$gen(500) %>%
  mutate(e_dist = "lognormal")

dat_shape_4 <- phn_model(include_z = FALSE, include_x2 = FALSE, e = rand_uniform_d(-1.4, 1.4, even = TRUE))$gen(500) %>%
  mutate(e_dist = "discrete")

# Generate data for shape 2, 3 and 4. Reuse x and e.
bind_rows(dat_shape_1, dat_shape_2, dat_shape_3, dat_shape_4) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~e_dist, scales = "free", labeller = label_parsed, ncol = 2)
```

Non-normality violations arise from specifying a non-normal distribution for $\boldsymbol{\varepsilon}$. In the synthetic data simulation, four distinct error distributions are considered, including discrete, uniform, normal, and lognormal distributions, as presented in Figure \ref{fig:different-e}. Each distribution imparts unique characteristics in the residual plot. The discrete error distribution introduces discrete clusters in residuals, while the lognormal distribution typically yields outliers. Uniform error distribution may result in residuals filling the entire space of the residual plot. All of these distributions exhibit visual distinctions from the normal error distribution.

```{r different-j-x2, fig.pos="!h", fig.cap = "Residual plots of multiple linear regression models with non-linearity issues. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$. A second predictor $\\boldsymbol{x}_2$ is introduced to the regression model to create complex shapes."}

set.seed(10086)

# Data for shape 1
dat_shape_1 <- phn_model(j = 2, include_x2 = TRUE, sigma = 0.05)$gen(500) %>%
  mutate(j = 2)

# Generate data for shape 2, 3 and 4. Reuse x and e.
map_df(3:18, function(j) {
  phn_model(j = j, include_x2 = TRUE, sigma = 0.05)$
    gen(500, computed = select(dat_shape_1, x1, e)) %>%
  mutate(j = j)
}) %>%
  
  # Combined with data for shape 1
  bind_rows(dat_shape_1) %>%
  mutate(j = factor(j)) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~j, scales = "free", labeller = label_parsed, ncol = 5)
```

```{r different-j-heter, fig.pos="!h", fig.cap = 'Residual plots of models violating both the non-linearity and the heteroskedasticity assumptions. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$, and the "left-triangle" shape is introduced by setting $a = -1$.'}

set.seed(10086)

# Data for shape 1
dat_shape_1 <- phn_model(j = 2, a = -1, b = 100, include_x2 = FALSE, sigma = 0.05)$gen(500) %>%
  mutate(j = 2)

# Generate data for shape 2, 3 and 4. Reuse x and e.
map_df(3:18, function(j) {
  phn_model(j = j, a = -1, b = 100, include_x2 = FALSE, sigma = 0.05)$
    gen(500, computed = select(dat_shape_1, x1, e)) %>%
  mutate(j = j)
}) %>%
  
  # Combined with data for shape 1
  bind_rows(dat_shape_1) %>%
  mutate(j = factor(j)) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~j, scales = "free", labeller = label_parsed, ncol = 5)
```


```{r different-e-heter, fig.pos="!h", fig.cap = 'Residual plots of models violating both the non-normality and the heteroskedasticity assumptions. The four shapes are generated by using four different error distributions including discrete, lognormal, normal and uniform, and the "left-triangle" shape is introduced by setting $a = -1$. '}

set.seed(10085)

# Data for shape 1
dat_shape_1 <- phn_model(a = -1, b = 100, include_z = FALSE, include_x2 = FALSE, e = rand_uniform(-1.4, 1.4))$gen(500) %>%
  mutate(e_dist = "uniform")

dat_shape_2 <- phn_model(a = -1, b = 100, include_z = FALSE, include_x2 = FALSE, sigma = 0.8)$gen(500) %>%
  mutate(e_dist = "normal")

dat_shape_3 <- phn_model(a = -1, b = 100, include_z = FALSE, include_x2 = FALSE, e = rand_lognormal(sigma = 0.6))$gen(500) %>%
  mutate(e_dist = "lognormal")

dat_shape_4 <- phn_model(a = -1, b = 100, include_z = FALSE, include_x2 = FALSE, e = rand_uniform_d(-1.4, 1.4, even = TRUE))$gen(500) %>%
  mutate(e_dist = "discrete")

# Generate data for shape 2, 3 and 4. Reuse x and e.
bind_rows(dat_shape_1, dat_shape_2, dat_shape_3, dat_shape_4) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~e_dist, scales = "free", labeller = label_parsed, ncol = 2)
```

Equation \ref{eq:data-sim} accommodates the incorporation of the second predictor $\boldsymbol{x}_2$. Introducing it into the data generation process by setting $\beta_1 = 1$ significantly enhances the complexity of the shapes, as illustrated in Figure \ref{fig:different-j-x2}. In comparison to Figure \ref{fig:different-j}, Figure \ref{fig:different-j-x2} demonstrates that the non-linear shape resembles a surface rather than a single curve. This augmentation can facilitate the computer vision model in learning visual patterns from residual plots of the multiple linear regression model.

In real-world analysis, it is not uncommon to encounter instances where multiple model violations coexist. In such cases, the residual plots often exhibit a mixed pattern of visual anomalies corresponding to different types of model violations. Figure \ref{fig:different-j-heter} and Figure \ref{fig:different-e-heter} show the visual patterns of models with multiple model violations.

The predictors, $\boldsymbol{x}_1$ and $\boldsymbol{x}_2$, are randomly generated from four distinct distributions, including $U(-1, 1)$ (uniform), $N(0, 0.3^2)$ (normal), $\text{lognormal}(0, 0.6^2)/3$ (skewed) and $U\{-1, 1\}$ (discrete uniform). 


## Balanced Dataset

To train a robust computer vision model, we deliberately controlled the distribution of the target variable $D$ in the training data. We ensured that it followed a uniform distribution between $0$ and $7$. This was achieved by organizing $50$ buckets, each exclusively accepting training samples with $D$ falling within the range $[7(i - 1)/49, 7i/49)$ for $i < 50$, where $i$ represents the index of the $i$-th bucket. For the $50$-th bucket, any training samples with $D \geq 7$ were accepted.

With 80,000 training images prepared, each bucket accommodated a maximum of $80000/ 50 = 1600$ training samples. The simulator iteratively sampled parameter values from the parameter space, generated residuals and fitted values using the data generation process, computed the distance, and checked if the sample fitted within the corresponding bucket. This process continued until all buckets were filled.

Similarly, we adopted the same methodology to prepare 8,000 test images for performance evaluation and model diagnostics.




# Neural Network Layers Used in the Study

This study used seven types of neural network layers, all of which are standard components frequently found in modern deep learning models. These layers are well-documented in textbooks like @goodfellow2016deep and @chollet2021deep, which offer thorough explanations and mathematical insights. In this section, we will offer a concise overview of these layers, drawing primarily from the insights provided in @goodfellow2016deep.

## Dense Layer

The Dense layer, also known as the fully-connected layer, is the fundamental unit in neural networks. It conducts a matrix multiplication operation between the input matrix $\boldsymbol{I}$ and a weight matrix $\boldsymbol{W}$ to generate the output matrix $\boldsymbol{O}$, which can be written as

$$\boldsymbol{O} = \boldsymbol{I}\boldsymbol{W} + b,$$

where $b$ is the intercept.

## ReLu Layer

The ReLU layer, short for rectified linear unit, is an element-wise non-linear function introduced by @nair2010rectified. It sets the output elements to zero if the corresponding input element is negative; otherwise, it retains the original input. Mathematically, it can be expressed as:

$$\boldsymbol{O}(i,j) = max\{0, \boldsymbol{I}(i,j)\},$$

where $\boldsymbol{O}(i,j)$ is the $i$th row and $j$th column entry of matrix $\boldsymbol{O}$, and $\boldsymbol{I}(i,j)$ is the $i$th row and $j$th column entry of matrix $\boldsymbol{I}$.

## Convolutional Layer

In Dense layers, matrix multiplication leads to each output unit interacting with every input unit, whereas convolutional layers operate differently with sparse interactions. An output unit in a convolutional layer is connected solely to a subset of input units, and the weight is shared across all input units. Achieving this involves using a kernel, typically a small square matrix, to conduct matrix multiplication across all input units. Precisely, this concept can be formulated as:

$$\boldsymbol{O}(i, j) = \sum_m\sum_n\boldsymbol{I}(i - m, j - n)K(m, n),$$

where $m$ and $n$ are the row and columns indices of the kernel $K$. 

If there are multiple kernels used in one covolutional layer, then each kernel will have its own weights and the output will be a three-dimensional tensor, where the length of the third channel is the number of kernels. 

## Pooling Layer

A pooling layer substitutes the input at a specific location with a summary statistic derived from nearby input units. Typically, there are two types of pooling layers: max pooling and average pooling. Max pooling computes the maximum value within a rectangular neighborhood, while average pooling calculates their average. Pooling layers helps making the representation approximately invariant to minor translations of the input. The output matrix of a pooling layer is approximately $s$ times smaller than the input matrix, where $s$ represents the length of the rectangular area. A max pooling layer can be formulated as:

$$\boldsymbol{O}(i, j) = \underset{m,n}{\max} \boldsymbol{I}(si + m,sj+n).$$

## Global Pooling Layer

A global pooling layer condenses an input matrix into a scalar value by either extracting the maximum or computing the average across all elements. This layer acts as a crucial link between the convolutional structure and the subsequent dense layers in a neural network architecture. When convolutional layers uses multiple kernels, the output becomes a three-dimensional tensor with numerous channels. In this scenario, the global pooling layer treats each channel individually, much like distinct features in a conventional classifier. This approach facilitates the extraction of essential features from complex data representations, enhancing the network's ability to learn meaningful patterns. A global max pooling layer can be formulated as 

$$O(i, j) = \underset{m,n,k}{\max} I(si + m,sj+n,k),$$
where $k$ is the kernel index.

## Batch Normalization Layer

Batch normalization is a method of adaptive reparametrization. One of the issues it adjusts is the simultaneous update of parameters in different layers, especially for network with a large number layers. At training time, the batch normalization layer normalizes the input matrix $I$ using the formula

$$\boldsymbol{O} = \frac{\boldsymbol{I} - \boldsymbol{\mu}_I}{\boldsymbol{\sigma}_I},$$

where $\boldsymbol{\mu}_I$ and $\boldsymbol{\sigma}_I$ are the mean and the standard deviation of each unit respectively.

It reparametrizes the model to make some units always be standardized by definition, such that the model training is stabilized. At inference time, $\boldsymbol{\mu}_I$ and $\boldsymbol{\sigma}_I$ are usually replaced with the running mean and running average obtained during training. 

## Dropout Layer

Dropout is a computationally inexpensive way to apply regularization on neural network. For each input unit, it randomly sets to be zero during training, effectively training a large number of subnetworks simultaneously, but these subnetworks share weights and each will only be trained for a single steps in a large network. It is essentially a different implementation of the bagging algorithm. Mathematically, it is formulated as 

$$\boldsymbol{O}(i,j) = \boldsymbol{D}(i,j)\boldsymbol{I}(i,j),$$

where $\boldsymbol{D}(i,j) \sim B(1, p)$ and $p$ is a hyperparameter that can be tuned.



# Model Training {#sec-model-training}

To achieve a near-optimal deep learning model, hyperparameters like the learning rate often need to be fine-tuned using a tuner. In our study, we utilized the Bayesian optimization tuner from the `KerasTuner` Python library [@omalley2019kerastuner] for this purpose. A comprehensive list of hyperparameters is provided in Table \ref{tab:hyperparameter}.

The number of base filters determines the number of filters for the first 2D convolutional layer. In the VGG16 architecture, the number of filters for the 2D convolutional layer in a block is typically twice the number in the previous block, except for the last block, which maintains the same number of convolution filters as the previous one. This hyperparameter aids in controlling the complexity of the computer vision model. A higher number of base filters results in more trainable parameters. Likewise, the number of units for the fully-connected layer determines the complexity of the final prediction block. Increasing the number of units enhances model complexity, resulting in more trainable parameters.

The dropout rate and batch normalization are flexible hyperparameters that work in conjunction with other parameters to facilitate smooth training. A higher dropout rate is necessary when the model tends to overfit the training dataset by learning too much noise [@srivastava2014dropout]. Conversely, a lower dropout rate is preferred when the model is complex and challenging to converge. Batch normalization, on the other hand, addresses the internal covariate shift problem arising from the randomness in weight initialization and input data [@goodfellow2016deep]. It helps stabilize and accelerate the training process by normalizing the activations of each layer.

Additionally, incorporating additional inputs such as scagnostics and the number of observations can potentially enhance prediction accuracy. Therefore, we allow the tuner to determine whether these inputs were necessary for optimal model performance.

The learning rate is a crucial hyperparameter, as it dictates the step size of the optimization algorithm. A high learning rate can help the model avoid local minima but risks overshooting and missing the global minimum. Conversely, a low learning rate smoothens the training process but makes the convergence time longer and increases the likelihood of getting trapped in local minima.

Our model was trained on the MASSIVE M3 high-performance computing platform [@goscinski2014multi], using TensorFlow [@abadi2016tensorflow] and Keras [@chollet2015keras]. During training, 80% of the training data was utilized for actual training, while the remaining 20% was used as validation data. The Bayesian optimization tuner conducted 100 trials to identify the best hyperparameter values based on validation root mean square error. The tuner then restored the best epoch of the best model from the trials. Additionally, we applied early stopping, terminating the training process if the validation root mean square error fails to improve for 50 epochs. The maximum allowed epochs was set at 2,000, although no models reached this threshold.

```{r}
data.frame(`Hyperparameter` = c("Number of base filters", "Dropout rate for convolutional blocks", "Batch normalization for convolutional blocks", "Type of global pooling", "Ignore additional inputs", "Number of units for the fully-connected layer", "Batch normalization for the fully-connected layer", "Dropout rate for the fully-connected layer", "Learning rate"),
           Domain = c("\\{4, 8, 16, 32, 64\\}", "[0.1, 0.6]", "\\{false, true\\}", "\\{max, average\\}", "\\{false, true\\}", "\\{128, 256, 512, 1024, 2048\\}", "\\{false, true\\}", "[0.1, 0.6]", "[$10^{-8}$, $10^{-1}$]")) %>%
  kableExtra::kable(format = "latex",
                    booktabs = TRUE, 
                    label = "hyperparameter",
                    caption = "Name of hyperparameters and their correspoding domain for the computer vision model.",
                    escape = FALSE)
```


Based on the tuning process described above, the optimized hyperparameter values are presented in Table \ref{tab:best-hyperparameter}. It was observable that a minimum of $32$ base filters was necessary, with the preferable choice being $64$ base filters for both the $64 \times 64$ and $128 \times 128$ models, mirroring the original VGG16 architecture. The optimized dropout rate for convolutional blocks hovered around $0.4$, and incorporating batch normalization for convolutional blocks proved beneficial for performance.

All optimized models chose to retain the additional inputs, contributing to the reduction of validation error. The number of units required for the fully-connected layer was $256$, a relatively modest number compared to the VGG16 classifier, suggesting that the problem at hand was less complex. The optimized learning rates were higher for models with higher resolution input, likely because models with more parameters are more prone to getting stuck in local minima, requiring a higher learning rate.

```{r}
data.frame(`Hyperparameter` = c("Number of base filters", "Dropout rate for convolutional blocks", "Batch normalization for convolutional blocks", "Type of global pooling", "Ignore additional inputs", "Number of units for the fully-connected layer", "Batch normalization for the fully-connected layer", "Dropout rate for the fully-connected layer", "Learning rate"),
           `32` = c("32", "0.4", "true", "max", "false", "256", "false", "0.2", "0.0003"),
           `64` = c("64", "0.3", "true", "average", "false", "256", "true", "0.4", "0.0006"),
           `128` = c("64", "0.4", "true", "average", "false", "256", "true", "0.1", "0.0052")) %>%
  kableExtra::kable(format = "latex",
                    booktabs = TRUE, 
                    label = "best-hyperparameter",
                    caption = "Hyperparameters values for the optimized computer vision models with different input sizes.",
                    col.names = c("Hyperparameter", "$32 \\times 32$", "$64 \\times 64$", "$128 \\times 128$"),
                    escape = FALSE) %>%
  kableExtra::kable_styling(latex_options = c("scale_down"))
```


# Model Violations Index {#sec-model-violations-index}

In Section 5.1, we noted that a pre-computed lattice of $\hat{D}$ quantiles can reduce the computation cost of lineup tests. Another practical approach is to assess model performance directly using the value of $\hat{D}$.

The estimator $\hat{D}$ captures the difference between the true and reference residual distributions, which reflects the extent of model violations, making it instrumental in forming a model violations index (MVI). However, when more observations are used in regression, the value of $\hat{D}$ tends to increase logarithmically. This is because $D = \log(1 + D_{KL})$, and under the assumption of independence, $D_{KL}$ is the sum of $D_{KL}^{(i)}$ across all observations. This does not mean that $\hat{D}$ becomes less reliable. In fact, larger samples often make model violations more visible in residual plots, unless strong overlapping masks the patterns.

However, to create a standardized and generalizable index, it is important to adjust for the effect of sample size. Therefore, the Model Violations Index (MVI) is proposed as

\begin{equation} \label{eq:mvi}
\text{MVI} = C + \hat{D} - \log(n),
\end{equation}

\noindent where $C$ is a sufficiently large constant to ensure the result remains positive, and the $\log(n)$ term offset the increase in $D$ with larger sample sizes.

Figure \ref{fig:poly-heter-index} displays the residual plots for fitted models exhibiting varying degrees of non-linearity and heteroskedasticity. Each residual plot's MVI is computed using Equation \ref{eq:mvi} with $C = 10$. When $\text{MVI} > 8$, the visual patterns are notably strong and easily discernible by humans. In the range $6 < \text{MVI} < 8$, the visibility of the visual pattern diminishes as MVI decreases. Conversely, when $\text{MVI} < 6$, the visual pattern tends to become relatively faint and challenging to observe. Table \ref{tab:mvi} provides a summary of the MVI usage and it is applicable to other linear regression models.


```{r}
data.frame(degree = c("Strong", "Moderate", "Weak"),
           range = c("$\\text{MVI} > 8$", "$6 < \\text{MVI} < 8$", "$\\text{MVI} < 6$")) %>%
  kableExtra::kable(format = "latex",
                    booktabs = TRUE,
                    label = "mvi",
                    caption = "Degree of model violations or the strength of the visual signals according to the Model Violations Index (MVI). The constant $C$ is set to be 10.",
                    escape = FALSE,
                    col.names = c("Degree of model violations", "Range ($C$ = 10)"),
                    align = "lc")
```


```{r}
set.seed(1495)
if (!file.exists(here::here("cached_data/poly_index.rds"))) {
  e_sigma_factor <- 3
  keras_mod <- autovi::get_keras_model("vss_phn_32")
  poly_dgp <- phn_model(j = 3, sigma = 0.1^e_sigma_factor)
  ori_dat <- poly_dgp$gen(300)
  
  vss <- c()
  actual_ss <- c()
  dat_combined <- tibble()
    
  for (e_sigma in seq(0.7, 1.8, length.out = 20)^e_sigma_factor) {
    poly_dgp <- phn_model(j = 3, sigma = e_sigma)
    dat <- poly_dgp$gen(300, computed = list(x1 = ori_dat$x1, e = ori_dat$e, k = ori_dat$k * e_sigma / 0.1^e_sigma_factor))
    mod <- lm(y ~ x1, data = dat)
    my_vi <- autovi::auto_vi(fitted_mod = mod, keras_mod = keras_mod)
    vss <- c(vss, my_vi$vss()$vss)
    actual_ss <- c(actual_ss, log(poly_dgp$sample_effect_size(dat) + 1))
    
    dat_combined <- bind_rows(dat_combined, mutate(dat, e_sigma = e_sigma))
  }
  
  saveRDS(dat_combined, here::here("cached_data/poly_index.rds"))
  saveRDS(vss, here::here("cached_data/poly_index_vss.rds"))
}

dat_combined <- readRDS(here::here("cached_data/poly_index.rds"))
vss <- readRDS(here::here("cached_data/poly_index_vss.rds"))

p1 <- dat_combined %>%
  autovi::AUTO_VI$plot_resid() +
  facet_wrap(~e_sigma, scales = "free", 
             labeller = function(...){ 
               tibble(nb = map(glue("'MVI' == {format(vss - log(300) + 10, digits = 3)}"), str2expression))
               },
             ncol = 5) +
  theme(strip.text = element_text(size = 10), plot.title = element_text(size = 10)) +
  ggtitle("(A) Non-linearity")
```

```{r}
set.seed(1495)
if (!file.exists(here::here("cached_data/heter_index.rds"))) {
  b_factor <- 2
  keras_mod <- autovi::get_keras_model("vss_phn_32")
  heter_dgp <- phn_model(include_z = FALSE, b = 0)
  ori_dat <- poly_dgp$gen(300)
  
  vss <- c()
  actual_ss <- c()
  dat_combined <- tibble()
    
  for (b in rev(seq(0.2, 4, length.out = 20))^b_factor) {
    heter_dgp <- phn_model(include_z = FALSE, b = b)
    dat <- heter_dgp$gen(300, computed = list(x1 = ori_dat$x1, e = ori_dat$e))
    mod <- lm(y ~ x1, data = dat)
    my_vi <- autovi::auto_vi(fitted_mod = mod, keras_mod = keras_mod)
    vss <- c(vss, my_vi$vss()$vss)
    actual_ss <- c(actual_ss, log(heter_dgp$sample_effect_size(dat) + 1))
    
    dat_combined <- bind_rows(dat_combined, mutate(dat, nb = -b))
  }
  
  saveRDS(dat_combined, here::here("cached_data/heter_index.rds"))
  saveRDS(vss, here::here("cached_data/heter_index_vss.rds"))
}

dat_combined <- readRDS(here::here("cached_data/heter_index.rds"))
vss <- readRDS(here::here("cached_data/heter_index_vss.rds"))

p2 <- dat_combined %>%
  autovi::AUTO_VI$plot_resid() +
  facet_wrap(~nb, scales = "free", 
             labeller = function(...){ 
               tibble(nb = map(glue("'MVI' == {format(vss - log(300) + 10, digits = 3)}"), str2expression))
               },
             ncol = 5) +
  theme(strip.text = element_text(size = 10), plot.title = element_text(size = 10)) +
  ggtitle("(B) Heteroskedasticity")
```

```{r poly-heter-index, fig.pos = "!h", fig.cap = "Residual plots generated from fitted models exhibiting varying degrees of (A) non-linearity and (B) heteroskedasticity violations. The model violations index (MVI) is displayed atop each residual plot. The non-linearity patterns are relatively strong for $MVI > 8$, and relatively weak for $MVI < 6$, while the heteroskedasticity patterns are relatively strong for $MVI > 8$, and relatively weak for $MVI < 6$.", fig.height = 10}
patchwork::wrap_plots(p1, p2, ncol = 1)
```



\clearpage
