\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{belsley1980regression,cook1982residuals}
\citation{li2024plot}
\citation{buja2009statistical}
\citation{langsrud2005rotation}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec-model-introduction}{{1}{2}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An example residual vs fitted values plot (red line indicates 0 corresponds to the x-intercept, i.e. $y=0$). The vertical spread of the data points varies with the fitted values. This often indicates the existence of heteroskedasticity, however, here the result is due to skewed distribution of the predictors rather than heteroskedasticity. The Breusch-Pagan test rejects this residual plot at 95\% significance level ($p\text  {-value} = 0.046$).}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:false-finding}{{1}{2}{An example residual vs fitted values plot (red line indicates 0 corresponds to the x-intercept, i.e. $y=0$). The vertical spread of the data points varies with the fitted values. This often indicates the existence of heteroskedasticity, however, here the result is due to skewed distribution of the predictors rather than heteroskedasticity. The Breusch-Pagan test rejects this residual plot at 95\% significance level ($p\text {-value} = 0.046$)}{figure.1}{}}
\citation{loy2013diagnostic}
\citation{loy2014hlmdiag}
\citation{loy2015you}
\citation{widen2016graphical}
\citation{fieberg2024using}
\citation{krishnan2021hierarchical}
\citation{li2024plot}
\citation{tukey1985computer}
\citation{wilkinson2005graph}
\citation{buja2009statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example lineup embedding the true residual plot among four null plots. In practice, lineups typically include 19 null plots, but a reduced set is shown here for presentation purposes. The null plots are generated via residual rotation to ensure consistency with $H_0$. Observers who have not previously seen the lineup are asked to identify the plot that appears most different. Under $H_0$ that the regression model is correctly specified, the true residual plot should be indistinguishable from the null plots, yielding a selection probability of 0.2. A small $p$-value arises when a substantial proportion of observers select the true residual plot (shown at position 2, exhibiting non-linearity).}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:ex-lineup}{{2}{3}{An example lineup embedding the true residual plot among four null plots. In practice, lineups typically include 19 null plots, but a reduced set is shown here for presentation purposes. The null plots are generated via residual rotation to ensure consistency with $H_0$. Observers who have not previously seen the lineup are asked to identify the plot that appears most different. Under $H_0$ that the regression model is correctly specified, the true residual plot should be indistinguishable from the null plots, yielding a selection probability of 0.2. A small $p$-value arises when a substantial proportion of observers select the true residual plot (shown at position 2, exhibiting non-linearity)}{figure.2}{}}
\citation{fukushima1982neocognitron}
\citation{rawat2017deep}
\citation{lee2015image}
\citation{brunetti2018computer}
\citation{emami2012facial}
\citation{ojeda2020multivariate}
\citation{chu2019automatic,hailesilassie2019financial,hatami2018classification,zhang2020encoding}
\citation{chen2020convolutional}
\citation{singh2017deep}
\citation{abadi2016tensorflow}
\citation{simonyan2014very}
\citation{chopra2005learning}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model Specifications}{5}{section.2}\protected@file@percent }
\newlabel{sec-model-specifications}{{2}{5}{Model Specifications}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Input Formats}{5}{subsection.2.1}\protected@file@percent }
\newlabel{input-formats}{{2.1}{5}{Input Formats}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Output Formats}{6}{subsection.2.2}\protected@file@percent }
\newlabel{output-formats}{{2.2}{6}{Output Formats}{subsection.2.2}{}}
\citation{wang2004image}
\citation{chowdhury2018measuring}
\@writefile{toc}{\contentsline {section}{\numberline {3}Distance from a Theoretically ``Good'' Residual Plot}{7}{section.3}\protected@file@percent }
\newlabel{sec-model-distance-between-residual-plots}{{3}{7}{Distance from a Theoretically ``Good'' Residual Plot}{section.3}{}}
\citation{frisch1933partial}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Residual Distribution}{8}{subsection.3.1}\protected@file@percent }
\newlabel{residual-distribution}{{3.1}{8}{Residual Distribution}{subsection.3.1}{}}
\citation{kullback1951information}
\citation{li2024plot}
\citation{li2024plot}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Distance of \(P\) from \(Q\)}{9}{subsection.3.2}\protected@file@percent }
\newlabel{sec-distance-of-p-from-q}{{3.2}{9}{\texorpdfstring {Distance of \(P\) from \(Q\)}{Distance of P from Q}}{subsection.3.2}{}}
\newlabel{eq:kl-0}{{1}{9}{\texorpdfstring {Distance of \(P\) from \(Q\)}{Distance of P from Q}}{equation.1}{}}
\newlabel{eq:kl-1}{{2}{9}{\texorpdfstring {Distance of \(P\) from \(Q\)}{Distance of P from Q}}{equation.2}{}}
\citation{li2024plot}
\citation{silverman2018density}
\newlabel{eq:kl-2}{{3}{10}{\texorpdfstring {Distance of \(P\) from \(Q\)}{Distance of P from Q}}{equation.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Non-normal \(P\)}{10}{subsection.3.3}\protected@file@percent }
\newlabel{non-normal-p}{{3.3}{10}{\texorpdfstring {Non-normal \(P\)}{Non-normal P}}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Distance Estimation}{11}{section.4}\protected@file@percent }
\newlabel{sec-model-distance-estimation}{{4}{11}{Distance Estimation}{section.4}{}}
\newlabel{eq:d-approx}{{4}{12}{Distance Estimation}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Statistical Testing}{12}{section.5}\protected@file@percent }
\newlabel{sec-model-statistical-testing}{{5}{12}{Statistical Testing}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Lineup Evaluation}{12}{subsection.5.1}\protected@file@percent }
\newlabel{sec-model-lineup-evaluation}{{5.1}{12}{Lineup Evaluation}{subsection.5.1}{}}
\citation{hyndman1996sample}
\citation{davison1997bootstrap,efron1994introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Bootstrapping}{13}{subsection.5.2}\protected@file@percent }
\newlabel{bootstrapping}{{5.2}{13}{Bootstrapping}{subsection.5.2}{}}
\citation{simonyan2014very}
\citation{he2016deep}
\citation{huang2017densely}
\citation{series2011studio}
\@writefile{toc}{\contentsline {section}{\numberline {6}Model Architecture}{14}{section.6}\protected@file@percent }
\newlabel{sec-model-architecture}{{6}{14}{Model Architecture}{section.6}{}}
\citation{mason2022cassowaryr}
\citation{Albrecht2023interp}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Diagram of the architecture of the optimized computer vision model. Numbers at the bottom of each box show the shape of the output of each layer. The band of each box drawn in a darker color indicates the use of the rectified linear unit activation function. Yellow boxes are 2D convolutional layers, orange boxes are pooling layers, the grey box is the concatenation layer, and the purple boxes are dense layers.}}{15}{figure.3}\protected@file@percent }
\newlabel{fig:cnn-diag}{{3}{15}{Diagram of the architecture of the optimized computer vision model. Numbers at the bottom of each box show the shape of the output of each layer. The band of each box drawn in a darker color indicates the use of the rectified linear unit activation function. Yellow boxes are 2D convolutional layers, orange boxes are pooling layers, the grey box is the concatenation layer, and the purple boxes are dense layers}{figure.3}{}}
\citation{glorot2010understanding}
\citation{kingma2014adam}
\citation{nikolenko2021synthetic}
\citation{hermite1864nouveau}
\@writefile{toc}{\contentsline {section}{\numberline {7}Data Generation and Model Training}{16}{section.7}\protected@file@percent }
\newlabel{sec-model-data-generation}{{7}{16}{Data Generation and Model Training}{section.7}{}}
\citation{goscinski2014multi}
\citation{abadi2016tensorflow}
\citation{chollet2015keras}
\citation{omalley2019kerastuner}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of training and test samples for each model violation scenario, including cases with multiple simultaneous violations. Each sample consists of a residual plot as input and the corresponding distance $D$ as the target. Root mean square error (RMSE) values for the training and test sets are shown for each scenario. Details of the synthetic data generating process used to construct these samples are provided in Appendix A.}}{17}{table.1}\protected@file@percent }
\newlabel{tab:data-overall}{{1}{17}{Number of training and test samples for each model violation scenario, including cases with multiple simultaneous violations. Each sample consists of a residual plot as input and the corresponding distance $D$ as the target. Root mean square error (RMSE) values for the training and test sets are shown for each scenario. Details of the synthetic data generating process used to construct these samples are provided in Appendix A}{table.1}{}}
\citation{hastie2017generalized}
\citation{li2024plot}
\@writefile{toc}{\contentsline {section}{\numberline {8}Results}{18}{section.8}\protected@file@percent }
\newlabel{sec-model-results}{{8}{18}{Results}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Model Performance}{18}{subsection.8.1}\protected@file@percent }
\newlabel{model-performance}{{8.1}{18}{Model Performance}{subsection.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Comparison with Human Visual Inference and Conventional Tests}{18}{subsection.8.2}\protected@file@percent }
\newlabel{comparison-with-human-visual-inference-and-conventional-tests}{{8.2}{18}{Comparison with Human Visual Inference and Conventional Tests}{subsection.8.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Overview of the Human Subject Experiment}{18}{subsubsection.8.2.1}\protected@file@percent }
\newlabel{overview-of-the-human-subject-experiment}{{8.2.1}{18}{Overview of the Human Subject Experiment}{subsubsection.8.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The test performance of three optimized models with different input sizes. The metrics are computed by comparing the estimated distance $\hat  {D}$ (model output) and the target distance $D$ generated from the synthetic data model. RMSE represents the root mean squared error; $R^2$ is the squared correlation between the two quantities; MAE denotes the mean absolute error; and Huber loss refers to the average Huber loss computed over the test set.}}{19}{table.2}\protected@file@percent }
\newlabel{tab:performance}{{2}{19}{The test performance of three optimized models with different input sizes. The metrics are computed by comparing the estimated distance $\hat {D}$ (model output) and the target distance $D$ generated from the synthetic data model. RMSE represents the root mean squared error; $R^2$ is the squared correlation between the two quantities; MAE denotes the mean absolute error; and Huber loss refers to the average Huber loss computed over the test set}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Hexagonal heatmap for difference in $D$ and $\hat  {D}$ vs $D$ on test data for three optimized models with different input sizes. The brown lines are smoothing curves produced by fitting generalized additive models. Over-prediction and under-prediction can be observed for small $D$ and large $D$ respectively.}}{19}{figure.4}\protected@file@percent }
\newlabel{fig:model-performance}{{4}{19}{Hexagonal heatmap for difference in $D$ and $\hat {D}$ vs $D$ on test data for three optimized models with different input sizes. The brown lines are smoothing curves produced by fitting generalized additive models. Over-prediction and under-prediction can be observed for small $D$ and large $D$ respectively}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Scatter plots for difference in $D$ and $\hat  {D}$ vs $\sigma $ on test data for the $32 \times 32$ optimized model. The data is grouped by whether the regression has only non-linearity violation, and whether it includes a second predictor in the regression formula. The brown lines are smoothing curves produced by fitting generalized additive models. Under-prediction mainly occurs when the data-generating process has small $\sigma $, a second predictor, and only non-linearity as the model violation.}}{19}{figure.5}\protected@file@percent }
\newlabel{fig:over-under}{{5}{19}{Scatter plots for difference in $D$ and $\hat {D}$ vs $\sigma $ on test data for the $32 \times 32$ optimized model. The data is grouped by whether the regression has only non-linearity violation, and whether it includes a second predictor in the regression formula. The brown lines are smoothing curves produced by fitting generalized additive models. Under-prediction mainly occurs when the data-generating process has small $\sigma $, a second predictor, and only non-linearity as the model violation}{figure.5}{}}
\citation{li2024plot}
\citation{li2024plot}
\citation{ramsey1969tests}
\citation{breusch1979simple}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Model Performance on the Human-evaluated Data}{20}{subsubsection.8.2.2}\protected@file@percent }
\newlabel{model-performance-on-the-human-evaluated-data}{{8.2.2}{20}{Model Performance on the Human-evaluated Data}{subsubsection.8.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The performance of the $32 \times 32$ model on the data used in the human subject experiment.}}{21}{table.3}\protected@file@percent }
\newlabel{tab:experiment-performance}{{3}{21}{The performance of the $32 \times 32$ model on the data used in the human subject experiment}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Summary of the comparison of decisions made by computer vision model with decisions made by conventional tests and visual tests conducted by human.}}{21}{table.4}\protected@file@percent }
\newlabel{tab:human-conv-table}{{4}{21}{Summary of the comparison of decisions made by computer vision model with decisions made by conventional tests and visual tests conducted by human}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Rejection rate ($p$-value $\leq 0.05$) of computer vision models conditional on conventional tests on non-linearity (left) and heteroskedasticity (right) lineups displayed using a mosaic plot. When the conventional test fails to reject, the computer vision mostly fails to reject the same plot as well as indicated by the height of the top right yellow rectangle, but there are non negliable amount of plots where the conventional test rejects but the computer vision model fails to reject as indicated by the width of the top left yellow rectangle.}}{21}{figure.6}\protected@file@percent }
\newlabel{fig:conv-mosaic}{{6}{21}{Rejection rate ($p$-value $\leq 0.05$) of computer vision models conditional on conventional tests on non-linearity (left) and heteroskedasticity (right) lineups displayed using a mosaic plot. When the conventional test fails to reject, the computer vision mostly fails to reject the same plot as well as indicated by the height of the top right yellow rectangle, but there are non negliable amount of plots where the conventional test rejects but the computer vision model fails to reject as indicated by the width of the top left yellow rectangle}{figure.6}{}}
\citation{li2024plot}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Parallel coordinate plots of decisions made by computer vision model, conventional tests and visual tests made by human. All three agree in around 50\% of cases. The model rejects less often than conventional tests when humans do not, and rarely rejects when both others do not, indicating closer alignment with human judgment.}}{22}{figure.7}\protected@file@percent }
\newlabel{fig:pcp}{{7}{22}{Parallel coordinate plots of decisions made by computer vision model, conventional tests and visual tests made by human. All three agree in around 50\% of cases. The model rejects less often than conventional tests when humans do not, and rarely rejects when both others do not, indicating closer alignment with human judgment}{figure.7}{}}
\citation{chowdhury2018measuring}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of power of visual tests, conventional tests and the computer vision model. Marks along the x-axis at the bottom of the plot represent rejections made by each type of test. Marks at the top of the plot represent acceptances. Power curves are fitted by logistic regression models with no intercept but an offset equals to $\text  {log}(0.05/0.95)$. The model is less sensitive than conventional tests for small $D$ but similarly sensitive for large $D$. Visual tests are least sensitive overall. The model’s curve lies between those of conventional and visual tests.}}{23}{figure.8}\protected@file@percent }
\newlabel{fig:power}{{8}{23}{Comparison of power of visual tests, conventional tests and the computer vision model. Marks along the x-axis at the bottom of the plot represent rejections made by each type of test. Marks at the top of the plot represent acceptances. Power curves are fitted by logistic regression models with no intercept but an offset equals to $\text {log}(0.05/0.95)$. The model is less sensitive than conventional tests for small $D$ but similarly sensitive for large $D$. Visual tests are least sensitive overall. The model’s curve lies between those of conventional and visual tests}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Adjusted \(\delta \)-difference}{23}{subsubsection.8.2.3}\protected@file@percent }
\newlabel{adjusted-delta-difference}{{8.2.3}{23}{\texorpdfstring {Adjusted \(\delta \)-difference}{Adjusted \textbackslash delta-difference}}{subsubsection.8.2.3}{}}
\citation{harrison1978hedonic}
\citation{datasaurus}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A weighted detection rate vs adjusted $\delta $-difference plot. The brown line is smoothing curve produced by fitting generalized additive models. Detection generally increases with positive $\delta _{\text  {adj}}$, but variability and exceptions highlight the distance measure's imperfect alignment with human perception.}}{24}{figure.9}\protected@file@percent }
\newlabel{fig:delta}{{9}{24}{A weighted detection rate vs adjusted $\delta $-difference plot. The brown line is smoothing curve produced by fitting generalized additive models. Detection generally increases with positive $\delta _{\text {adj}}$, but variability and exceptions highlight the distance measure's imperfect alignment with human perception}{figure.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Examples}{25}{section.9}\protected@file@percent }
\newlabel{sec-examples}{{9}{25}{Examples}{section.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Left-triangle}{25}{subsection.9.1}\protected@file@percent }
\newlabel{left-triangle}{{9.1}{25}{Left-triangle}{subsection.9.1}{}}
\citation{harrison1978hedonic}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A summary of the residual plot assessment evaluated on 200 null plots and 200 bootstrapped plots. (A) The true residual plot exhibiting a "left-triangle" shape. (B) The attention map highlights the top-right and bottom-right corners of the residual plot as the most influential. (C) The density plot shows estimated distances for null (yellow) and bootstrapped (green) plots. The fitted model is not rejected because $\hat  {D} < Q_{null}(0.95)$. (D) Null and bootstrapped plots cluster together in the space defined by the first two principal components of the global pooling layer. The cluster also covers the true residual plot.}}{26}{figure.10}\protected@file@percent }
\newlabel{fig:false-check}{{10}{26}{A summary of the residual plot assessment evaluated on 200 null plots and 200 bootstrapped plots. (A) The true residual plot exhibiting a "left-triangle" shape. (B) The attention map highlights the top-right and bottom-right corners of the residual plot as the most influential. (C) The density plot shows estimated distances for null (yellow) and bootstrapped (green) plots. The fitted model is not rejected because $\hat {D} < Q_{null}(0.95)$. (D) Null and bootstrapped plots cluster together in the space defined by the first two principal components of the global pooling layer. The cluster also covers the true residual plot}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Boston Housing}{26}{subsection.9.2}\protected@file@percent }
\newlabel{boston-housing}{{9.2}{26}{Boston Housing}{subsection.9.2}{}}
\citation{shapiro1965analysis}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}DatasauRus}{27}{subsection.9.3}\protected@file@percent }
\newlabel{datasaurus}{{9.3}{27}{DatasauRus}{subsection.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A summary of the residual plot assessment for the Boston housing fitted model evaluated on 200 null plots and 200 bootstrapped plots. (A) The true residual plot exhibiting a "U" shape. (B) The attention map highlights the central region of the "U" shape as the most influential part of the residual plot. (C) The density plot shows estimated distances for null (yellow) and bootstrapped (green) plots. The fitted model is rejected because $\hat  {D} \geq Q_{null}(0.95)$. (D) Bootstrapped and null plots form distinct clusters in the space of the first two principal components from the global pooling layer, highlighting their visual separability.}}{28}{figure.11}\protected@file@percent }
\newlabel{fig:boston-check}{{11}{28}{A summary of the residual plot assessment for the Boston housing fitted model evaluated on 200 null plots and 200 bootstrapped plots. (A) The true residual plot exhibiting a "U" shape. (B) The attention map highlights the central region of the "U" shape as the most influential part of the residual plot. (C) The density plot shows estimated distances for null (yellow) and bootstrapped (green) plots. The fitted model is rejected because $\hat {D} \geq Q_{null}(0.95)$. (D) Bootstrapped and null plots form distinct clusters in the space of the first two principal components from the global pooling layer, highlighting their visual separability}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A summary of the residual plot assessment for the datasauRus fitted model evaluated on 200 null plots and 200 bootstrapped plots. (A) The residual plot exhibits a "dinosaur" shape. (B) The attention map highlights the dinosaur shape, indicating the model's decision is driven by human-recognizable features.(C) The density plot shows estimated distances for null (yellow) and bootstrapped (green) plots. The fitted model is rejected because $\hat  {D} \geq Q_{null}(0.95)$. (D) The bootstrapped plots cluster at the corner of the null plot cluster, yet remain isolated in the space defined by the first two principal components of the global pooling layer.}}{29}{figure.12}\protected@file@percent }
\newlabel{fig:dino-check}{{12}{29}{A summary of the residual plot assessment for the datasauRus fitted model evaluated on 200 null plots and 200 bootstrapped plots. (A) The residual plot exhibits a "dinosaur" shape. (B) The attention map highlights the dinosaur shape, indicating the model's decision is driven by human-recognizable features.(C) The density plot shows estimated distances for null (yellow) and bootstrapped (green) plots. The fitted model is rejected because $\hat {D} \geq Q_{null}(0.95)$. (D) The bootstrapped plots cluster at the corner of the null plot cluster, yet remain isolated in the space defined by the first two principal components of the global pooling layer}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Limitations and Future Work}{29}{section.10}\protected@file@percent }
\newlabel{limitations-and-future-work}{{10}{29}{Limitations and Future Work}{section.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Lineups of residual plots for the "left-triangle", "Boston housing", and "datasauRus" datasets. (A) True plot at position 10; no clear visual difference. (B) True plot at position 7; "U"-shape clearly stands out. (C) True plot at position 17; distinctly artificial and easily identifiable.}}{30}{figure.13}\protected@file@percent }
\newlabel{fig:lineup}{{13}{30}{Lineups of residual plots for the "left-triangle", "Boston housing", and "datasauRus" datasets. (A) True plot at position 10; no clear visual difference. (B) True plot at position 7; "U"-shape clearly stands out. (C) True plot at position 17; distinctly artificial and easily identifiable}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Conclusions}{31}{section.11}\protected@file@percent }
\newlabel{conclusions}{{11}{31}{Conclusions}{section.11}{}}
\newlabel{supplementary-materials}{{11}{31}{Supplementary Materials}{section*.1}{}}
\@writefile{toc}{\contentsline {section}{Supplementary Materials}{31}{section*.1}\protected@file@percent }
\citation{tidyverse}
\citation{lmtest}
\citation{mpoly}
\citation{ggmosaic}
\citation{kableextra}
\citation{patchwork}
\citation{glue}
\citation{ggpcp}
\citation{here}
\citation{reticulate}
\citation{rticles}
\citation{knitr}
\citation{rmarkdown}
\bibstyle{tfcad}
\bibdata{bibliography.bib}
\bibcite{abadi2016tensorflow}{{1}{2016}{{Abadi et~al.}}{{}}}
\bibcite{rticles}{{2}{2022}{{Allaire et~al.}}{{}}}
\newlabel{acknowledgement}{{11}{32}{Acknowledgement}{section*.2}{}}
\@writefile{toc}{\contentsline {section}{Acknowledgement}{32}{section*.2}\protected@file@percent }
\newlabel{data-availability-statement}{{11}{32}{Data Availability Statement}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{Data Availability Statement}{32}{section*.3}\protected@file@percent }
\newlabel{disclosure-statement}{{11}{32}{Disclosure Statement}{section*.4}{}}
\@writefile{toc}{\contentsline {section}{Disclosure Statement}{32}{section*.4}\protected@file@percent }
\bibcite{belsley1980regression}{{3}{1980}{{Belsley, Kuh, and Welsch}}{{}}}
\bibcite{breusch1979simple}{{4}{1979}{{Breusch and Pagan}}{{}}}
\bibcite{brunetti2018computer}{{5}{2018}{{Brunetti et~al.}}{{}}}
\bibcite{buja2009statistical}{{6}{2009}{{Buja et~al.}}{{}}}
\bibcite{chen2020convolutional}{{7}{2020}{{Chen, Su, and Yang}}{{}}}
\bibcite{chollet2015keras}{{8}{2015}{{Chollet et~al.}}{{}}}
\bibcite{chopra2005learning}{{9}{2005}{{Chopra, Hadsell, and LeCun}}{{}}}
\bibcite{chowdhury2018measuring}{{10}{2018}{{Chowdhury et~al.}}{{}}}
\bibcite{chu2019automatic}{{11}{2019}{{Chu et~al.}}{{}}}
\bibcite{cook1982residuals}{{12}{1982}{{Cook and Weisberg}}{{}}}
\bibcite{datasaurus}{{13}{2022}{{Davies, Locke, and {D'Agostino McGowan}}}{{}}}
\bibcite{davison1997bootstrap}{{14}{1997}{{Davison and Hinkley}}{{}}}
\bibcite{efron1994introduction}{{15}{1994}{{Efron and Tibshirani}}{{}}}
\bibcite{emami2012facial}{{16}{2012}{{Emami and Suciu}}{{}}}
\bibcite{fieberg2024using}{{17}{2024}{{Fieberg, Freeman, and Signer}}{{}}}
\bibcite{frisch1933partial}{{18}{1933}{{Frisch and Waugh}}{{}}}
\bibcite{fukushima1982neocognitron}{{19}{1982}{{Fukushima and Miyake}}{{}}}
\bibcite{Albrecht2023interp}{{20}{2023}{{Gebhardt, Bivand, and Sinclair}}{{}}}
\bibcite{glorot2010understanding}{{21}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{goscinski2014multi}{{22}{2014}{{Goscinski et~al.}}{{}}}
\bibcite{hailesilassie2019financial}{{23}{2019}{{Hailesilassie}}{{}}}
\bibcite{harrison1978hedonic}{{24}{1978}{{Harrison~Jr and Rubinfeld}}{{}}}
\bibcite{hastie2017generalized}{{25}{2017}{{Hastie}}{{}}}
\bibcite{hatami2018classification}{{26}{2018}{{Hatami, Gavet, and Debayle}}{{}}}
\bibcite{he2016deep}{{27}{2016}{{He et~al.}}{{}}}
\bibcite{hermite1864nouveau}{{28}{1864}{{Hermite}}{{}}}
\bibcite{glue}{{29}{2022}{{Hester and Bryan}}{{}}}
\bibcite{ggpcp}{{30}{2022}{{Hofmann, VanderPlas, and Ge}}{{}}}
\bibcite{huang2017densely}{{31}{2017}{{Huang et~al.}}{{}}}
\bibcite{hyndman1996sample}{{32}{1996}{{Hyndman and Fan}}{{}}}
\bibcite{ggmosaic}{{33}{2021}{{Jeppson, Hofmann, and Cook}}{{}}}
\bibcite{mpoly}{{34}{2013}{{Kahle}}{{}}}
\bibcite{kingma2014adam}{{35}{2014}{{Kingma and Ba}}{{}}}
\bibcite{krishnan2021hierarchical}{{36}{2021}{{Krishnan and Hofmann}}{{}}}
\bibcite{kullback1951information}{{37}{1951}{{Kullback and Leibler}}{{}}}
\bibcite{langsrud2005rotation}{{38}{2005}{{Langsrud}}{{}}}
\bibcite{lee2015image}{{39}{2015}{{Lee and Chen}}{{}}}
\bibcite{li2024plot}{{40}{2024}{{Li et~al.}}{{}}}
\bibcite{loy2013diagnostic}{{41}{2013}{{Loy and Hofmann}}{{}}}
\bibcite{loy2014hlmdiag}{{42}{2014}{{Loy and Hofmann}}{{}}}
\bibcite{loy2015you}{{43}{2015}{{Loy and Hofmann}}{{}}}
\bibcite{mason2022cassowaryr}{{44}{2022}{{Mason et~al.}}{{}}}
\bibcite{here}{{45}{2020}{{Müller}}{{}}}
\bibcite{nikolenko2021synthetic}{{46}{2021}{{Nikolenko et~al.}}{{}}}
\bibcite{ojeda2020multivariate}{{47}{2020}{{Ojeda, Solano, and Peramo}}{{}}}
\bibcite{omalley2019kerastuner}{{48}{2019}{{O'Malley et~al.}}{{}}}
\bibcite{patchwork}{{49}{2022}{{Pedersen}}{{}}}
\bibcite{ramsey1969tests}{{50}{1969}{{Ramsey}}{{}}}
\bibcite{rawat2017deep}{{51}{2017}{{Rawat and Wang}}{{}}}
\bibcite{series2011studio}{{52}{2011}{{Series}}{{}}}
\bibcite{shapiro1965analysis}{{53}{1965}{{Shapiro and Wilk}}{{}}}
\bibcite{silverman2018density}{{54}{2018}{{Silverman}}{{}}}
\bibcite{simonyan2014very}{{55}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{singh2017deep}{{56}{2017}{{Singh et~al.}}{{}}}
\bibcite{tukey1985computer}{{57}{1985}{{Tukey and Tukey}}{{}}}
\bibcite{reticulate}{{58}{2024}{{Ushey, Allaire, and Tang}}{{}}}
\bibcite{wang2004image}{{59}{2004}{{Wang et~al.}}{{}}}
\bibcite{tidyverse}{{60}{2019}{{Wickham et~al.}}{{}}}
\bibcite{widen2016graphical}{{61}{2016}{{Widen et~al.}}{{}}}
\bibcite{wilkinson2005graph}{{62}{2005}{{Wilkinson, Anand, and Grossman}}{{}}}
\bibcite{knitr}{{63}{2014}{{Xie}}{{}}}
\bibcite{rmarkdown}{{64}{2020}{{Xie, Dervieux, and Riederer}}{{}}}
\bibcite{lmtest}{{65}{2002}{{Zeileis and Hothorn}}{{}}}
\bibcite{zhang2020encoding}{{66}{2020}{{Zhang et~al.}}{{}}}
\bibcite{kableextra}{{67}{2021}{{Zhu}}{{}}}
\gdef \@abspage@last{37}
