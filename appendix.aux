\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {A}Data Generation}{1}{appendix.A}\protected@file@percent }
\newlabel{sec-model-data-generation}{{A}{1}{Data Generation}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Simulation Scheme}{1}{subsection.A.1}\protected@file@percent }
\newlabel{simulation-scheme}{{A.1}{1}{Simulation Scheme}{subsection.A.1}{}}
\citation{hermite1864nouveau}
\citation{li2024plot}
\@writefile{lot}{\contentsline {table}{\numberline {A1}{\ignorespaces Factors used in the data generating process for synthetic data simulation. Factor $j$ and $a$ controls the non-linearity shape and the heteroskedasticity shape respectively. Factor $b$, $\sigma _\varepsilon $ and $n$ control the signal strength. Factor $\text  {dist}_\varepsilon $, $\text  {dist}_{x1}$ and $\text  {dist}_{x2}$ specifies the distribution of $\varepsilon $, $X_1$ and $X_2$ respectively.}}{2}{table.A.1}\protected@file@percent }
\newlabel{tab:factor}{{A1}{2}{Factors used in the data generating process for synthetic data simulation. Factor $j$ and $a$ controls the non-linearity shape and the heteroskedasticity shape respectively. Factor $b$, $\sigma _\varepsilon $ and $n$ control the signal strength. Factor $\text {dist}_\varepsilon $, $\text {dist}_{x1}$ and $\text {dist}_{x2}$ specifies the distribution of $\varepsilon $, $X_1$ and $X_2$ respectively}{table.A.1}{}}
\newlabel{eq:data-sim}{{A1}{2}{Simulation Scheme}{equation.A.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A1}{\ignorespaces Non-linearity forms generated for the synthetic data simulation. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$.}}{3}{figure.A.1}\protected@file@percent }
\newlabel{fig:different-j}{{A1}{3}{Non-linearity forms generated for the synthetic data simulation. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$}{figure.A.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A2}{\ignorespaces Heteroskedasticity forms generated for the synthetic data simulation. Different shapes are controlled by the continuous factor $a$ between -1 and 1. For $a = -1$, the residual plot exhibits a "left-triangle" shape. And for $a = 1$, the residual plot exhibits a "right-triangle" shape. }}{4}{figure.A.2}\protected@file@percent }
\newlabel{fig:different-a}{{A2}{4}{Heteroskedasticity forms generated for the synthetic data simulation. Different shapes are controlled by the continuous factor $a$ between -1 and 1. For $a = -1$, the residual plot exhibits a "left-triangle" shape. And for $a = 1$, the residual plot exhibits a "right-triangle" shape}{figure.A.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A3}{\ignorespaces Non-normality forms generated for the synthetic data simulation. Four different error distributions including discrete, lognormal, normal and uniform are considered.}}{5}{figure.A.3}\protected@file@percent }
\newlabel{fig:different-e}{{A3}{5}{Non-normality forms generated for the synthetic data simulation. Four different error distributions including discrete, lognormal, normal and uniform are considered}{figure.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A4}{\ignorespaces Residual plots of multiple linear regression models with non-linearity issues. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$. A second predictor $\boldsymbol  {x}_2$ is introduced to the regression model to create complex shapes.}}{6}{figure.A.4}\protected@file@percent }
\newlabel{fig:different-j-x2}{{A4}{6}{Residual plots of multiple linear regression models with non-linearity issues. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$. A second predictor $\boldsymbol {x}_2$ is introduced to the regression model to create complex shapes}{figure.A.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Balanced Dataset}{6}{subsection.A.2}\protected@file@percent }
\newlabel{balanced-dataset}{{A.2}{6}{Balanced Dataset}{subsection.A.2}{}}
\citation{goodfellow2016deep}
\citation{chollet2021deep}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {A5}{\ignorespaces Residual plots of models violating both the non-linearity and the heteroskedasticity assumptions. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$, and the "left-triangle" shape is introduced by setting $a = -1$.}}{7}{figure.A.5}\protected@file@percent }
\newlabel{fig:different-j-heter}{{A5}{7}{Residual plots of models violating both the non-linearity and the heteroskedasticity assumptions. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$, and the "left-triangle" shape is introduced by setting $a = -1$}{figure.A.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Neural Network Layers Used in the Study}{7}{appendix.B}\protected@file@percent }
\newlabel{neural-network-layers-used-in-the-study}{{B}{7}{Neural Network Layers Used in the Study}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Dense Layer}{7}{subsection.B.1}\protected@file@percent }
\newlabel{dense-layer}{{B.1}{7}{Dense Layer}{subsection.B.1}{}}
\citation{nair2010rectified}
\@writefile{lof}{\contentsline {figure}{\numberline {A6}{\ignorespaces Residual plots of models violating both the non-normality and the heteroskedasticity assumptions. The four shapes are generated by using four different error distributions including discrete, lognormal, normal and uniform, and the "left-triangle" shape is introduced by setting $a = -1$. }}{8}{figure.A.6}\protected@file@percent }
\newlabel{fig:different-e-heter}{{A6}{8}{Residual plots of models violating both the non-normality and the heteroskedasticity assumptions. The four shapes are generated by using four different error distributions including discrete, lognormal, normal and uniform, and the "left-triangle" shape is introduced by setting $a = -1$}{figure.A.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}ReLu Layer}{8}{subsection.B.2}\protected@file@percent }
\newlabel{relu-layer}{{B.2}{8}{ReLu Layer}{subsection.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Convolutional Layer}{9}{subsection.B.3}\protected@file@percent }
\newlabel{convolutional-layer}{{B.3}{9}{Convolutional Layer}{subsection.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Pooling Layer}{9}{subsection.B.4}\protected@file@percent }
\newlabel{pooling-layer}{{B.4}{9}{Pooling Layer}{subsection.B.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Global Pooling Layer}{10}{subsection.B.5}\protected@file@percent }
\newlabel{global-pooling-layer}{{B.5}{10}{Global Pooling Layer}{subsection.B.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.6}Batch Normalization Layer}{10}{subsection.B.6}\protected@file@percent }
\newlabel{batch-normalization-layer}{{B.6}{10}{Batch Normalization Layer}{subsection.B.6}{}}
\citation{omalley2019kerastuner}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.7}Dropout Layer}{11}{subsection.B.7}\protected@file@percent }
\newlabel{dropout-layer}{{B.7}{11}{Dropout Layer}{subsection.B.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Model Training}{11}{appendix.C}\protected@file@percent }
\newlabel{sec-model-training}{{C}{11}{Model Training}{appendix.C}{}}
\citation{srivastava2014dropout}
\citation{goodfellow2016deep}
\citation{goscinski2014multi}
\citation{abadi2016tensorflow}
\citation{chollet2015keras}
\@writefile{lot}{\contentsline {table}{\numberline {C1}{\ignorespaces Name of hyperparameters and their correspoding domain for the computer vision model.}}{13}{table.C.1}\protected@file@percent }
\newlabel{tab:hyperparameter}{{C1}{13}{Name of hyperparameters and their correspoding domain for the computer vision model}{table.C.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C2}{\ignorespaces Hyperparameters values for the optimized computer vision models with different input sizes.}}{13}{table.C.2}\protected@file@percent }
\newlabel{tab:best-hyperparameter}{{C2}{13}{Hyperparameters values for the optimized computer vision models with different input sizes}{table.C.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Model Violations Index}{14}{appendix.D}\protected@file@percent }
\newlabel{sec-model-violations-index}{{D}{14}{Model Violations Index}{appendix.D}{}}
\newlabel{eq:mvi}{{D1}{14}{Model Violations Index}{equation.D.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {D1}{\ignorespaces Degree of model violations or the strength of the visual signals according to the Model Violations Index (MVI). The constant $C$ is set to be 10.}}{15}{table.D.1}\protected@file@percent }
\newlabel{tab:mvi}{{D1}{15}{Degree of model violations or the strength of the visual signals according to the Model Violations Index (MVI). The constant $C$ is set to be 10}{table.D.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {D1}{\ignorespaces Residual plots generated from fitted models exhibiting varying degrees of (A) non-linearity and (B) heteroskedasticity violations. The model violations index (MVI) is displayed atop each residual plot. The non-linearity patterns are relatively strong for $MVI > 8$, and relatively weak for $MVI < 6$, while the heteroskedasticity patterns are relatively strong for $MVI > 8$, and relatively weak for $MVI < 6$.}}{16}{figure.D.1}\protected@file@percent }
\newlabel{fig:poly-heter-index}{{D1}{16}{Residual plots generated from fitted models exhibiting varying degrees of (A) non-linearity and (B) heteroskedasticity violations. The model violations index (MVI) is displayed atop each residual plot. The non-linearity patterns are relatively strong for $MVI > 8$, and relatively weak for $MVI < 6$, while the heteroskedasticity patterns are relatively strong for $MVI > 8$, and relatively weak for $MVI < 6$}{figure.D.1}{}}
\bibstyle{tfcad}
\bibdata{bibliography.bib}
\bibcite{abadi2016tensorflow}{{1}{2016}{{Abadi et~al.}}{{}}}
\bibcite{chollet2015keras}{{2}{2015}{{Chollet et~al.}}{{}}}
\bibcite{chollet2021deep}{{3}{2021}{{Chollet}}{{}}}
\bibcite{goodfellow2016deep}{{4}{2016}{{Goodfellow, Bengio, and Courville}}{{}}}
\bibcite{goscinski2014multi}{{5}{2014}{{Goscinski et~al.}}{{}}}
\bibcite{hermite1864nouveau}{{6}{1864}{{Hermite}}{{}}}
\bibcite{li2024plot}{{7}{2024}{{Li et~al.}}{{}}}
\bibcite{nair2010rectified}{{8}{2010}{{Nair and Hinton}}{{}}}
\bibcite{omalley2019kerastuner}{{9}{2019}{{O'Malley et~al.}}{{}}}
\bibcite{srivastava2014dropout}{{10}{2014}{{Srivastava et~al.}}{{}}}
\gdef \@abspage@last{17}
