---
title: Automated Assessment of Residual Plots with Computer Vision Models
subtitle: Response to reviewers
date: today
filters:
   - latex-environment
environments: [reviewer]
commands: [deemph, review]
format: 
  pdf:
    include-in-header:
      text: | 
       \usepackage[dvipsnames]{xcolor}
       \newcommand{\review}[1]{{\color{violet} #1}}
       \newenvironment{reviewer}{
       \color{violet}}
       {}
       \renewenvironment{quote}{%
        \list{}{%
          \leftmargin0.5cm   % this is the adjusting screw
          \itshape
          \rightmargin\leftmargin
        }
        \item\relax
       }
       {\endlist}

---

We thank the editor and the reviewer for their constructive comments that have improved this paper. We have addressed all comments, colored in [purple]{.review}. In addition, we also include an annotated version of the paper (`diff.pdf`) with the difference between the original and revised versions produced using `latexdiff`. 

## Editor

::: reviewer

One referee, an Associate Editor (AE), and I have reviewed your paper. Although the topic is interesting, the referee raised several major concerns, which are detailed in their reports. The AE also indicated in the report that "the description of the work and the work itself require additional detail and rigor." As such, I cannot accept your paper in its current form. Nonetheless, given its potential, I would consider a revision that adequately addresses all the referees' comments.

:::

## Reviewer 

### General Comments

::: reviewer


The idea of the paper seems interesting: using computer vision to help identify residual plots which indicates model violation. For any computer vision task, the input is clearly the image. However, there should be a clear definition of the outcome variable that the algorithm wants to predict from images. From the current draft of this paper, it is unclear that what is the outcome variable that the computer vision is learning from the residual plots, how the training datasets are created for computer vision learning, and how the performance of the computer vision algorithm is assessed. Please see my detailed comments in items 10, 12,13. I suggest the authors make these basic setups explicitly defined from the very beginning of the article.

:::

XXX A new paragraph is written in the introduction section that gives an overview of the setup and goals.

### Specific Comments

::: reviewer

1. p3, l9: What is “the lineup protocol”?

:::

The lineup protocol is described on pages 2 and 3, with relevant citations. We have slightly revised the wording for clarity and added an example lineup figure to make the description easier to understand.

::: reviewer

2. missing references: Loy and Hofmann (2013; 2014; 2015)

:::

We are ignoring this comments, as the three references are already cited in our submission.

::: reviewer

3. p8 l16: Why do we need to replace it by a full-rank covariance matrix?

:::

We have added a new sentence to the corresponding paragraph, explaining that the covariance matrix is replaced with a full-rank diagonal matrix to ensure a non-degenerate distribution for KL divergence computation, and that more complex approaches, such as eigen-decomposition, are possible but the diagonal approximation is used for simplicity and computational efficiency.

::: reviewer

4. eq (2), typically KL is specified by KL(p—q) due to asymmetry

:::

We have modified $D_{KL}$ to $D_{KL}(P \| Q)$, thank you.

::: reviewer

5. p9 l 53: to solve eq 2: using “evaluate” for “solve” is better

:::

We have changed "solve" to "evaluate", thank you.

::: reviewer

6. p19 l30: What is “the data generating process”? We don’t know the true distribution of y.

:::

The data-generating process used for the human subject experiment is a special case of the process used to generate the training data for the computer vision model in this study. This process is described in Section 7, with additional details provided in the appendix. We have also revised the text in Section 7 to clarify how the synthetic dataset is generated and why it is required.

In brief, for data observed in practice, the true distribution of $y$ is unknown, and therefore the distance proposed in this study cannot be computed. In contrast, for synthetic data, we have precise control over the data-generating process, so the true distribution of $y$ is known. This allows us to generate pairs of residual plots together with their corresponding distances, which are used to train the computer vision model.

For the human subject experiment, we use the same data-generating process, but only present lineups of residual plots to participants in order to evaluate visual tests.

::: reviewer


7. p12, sec 5.1: this sounds like a standard simulation of the sampling distribution of $\hat{D}$ in traditional statistics.

:::

We agree that our approach follows standard simulation-based methodology for approximating sampling distributions of a test statistic. To clarify this and address the reviewer's comment, we have revised the paragraph to explicitly acknowledge this connection (Paragraph 3, Section 5.1).

::: reviewer

8. p13, Sec 5.2: How do you do bootstrapping? We need to know the distribution of ˆD under the null. However, the given observed data y may not come from the null.

:::

We use standard non-parametric bootstrapping, i.e., resampling the observed $\{(y_i, X_i)\}_{i=1}^n$ with replacement to form bootstrap samples, as stated in Section 5.2. For each bootstrap sample, the regression model is refitted and the corresponding residual plot is constructed, from which $\hat{D}$ is computed.

The null distribution of $\hat{D}$ is obtained separately from the bootstrap procedure. Specifically, assuming the fitted regression model is correct, we generate null residuals using residual rotation. These rotated residuals are used to construct null plots, which are then evaluated by the computer vision model to produce $\hat{D}_{null}$, forming an empirical approximation to the null distribution. In principle, this process could be repeated for each bootstrap sample to obtain a sample-specific $\hat{D}_{null}$. However, as discussed in Section 5.2, doing so would be computationally expensive.

We acknowledge that the observed $y$ may not arise from the null model. In that case, the regression model is misspecified. If this misspecification is reflected in the residual plot, we expect the observed $\hat{D}$ (from the true residual plot) to be larger than many values from the null distribution, depending on the strength of the underlying signal. This is the central idea of the paper, and the corresponding testing procedure is described in Section 5.1.

Importantly, the bootstrap does not assume that the observed $y$ comes from the null model. Its purpose is to quantify the sampling variability of $\hat{D}$ under the empirical data-generating mechanism and to assess how frequently bootstrap replicates would lead to rejection. The null distribution used for inference is generated via residual rotation, not via bootstrapping.


::: reviewer


9. Tbl1: it is unclear what this table is measuring. What is the $R^2$ measuring? What’s the response and what is the predictors?

:::

XXX We have expanded the caption to make this clearer.

::: reviewer


10. Section 7 and 8: In computing $\hat{D}$, you need a P and Q for each targeted model violation. Is your computer vision learning algorithm targeted a particular model departure, eg, non-linearity or heteroskedasticity? What is your P and Q in generating the training data of $\hat{D}$ and residual plots for computer vision learning? However, your results also show your performance for different kinds of model violations. This is confusing. You need to specify clearly what is the “true” $\hat{D}$ and what the “predicted $\hat{D}$” using computer vision in generating training data of $\hat{D}$ and residual plots.

:::

As stated in Section 7, “Importantly, the simulation included scenarios with multiple violations occurring simultaneously,” the synthetic data generating process allows for both single and combined model violations. These violations include non-linearity, heteroskedasticity, and non-normality. The computer vision model is therefore not designed to target a specific violation in isolation, but rather to learn from the full spectrum of possible departures represented in the simulated data. To clarify this point, we have revised the text in Section 7 and added a table summarising the number of training and test samples across the different violation scenarios.

In computing $\hat{D}$, the reference distribution $Q$ is always the residual distribution obtained by fitting a simple linear regression model under the standard assumptions, namely  

$$Q = N(\boldsymbol{0}_n, \mathrm{diag}(\boldsymbol{R}\sigma^2)).$$

The distribution $P$, by contrast, is determined by the synthetic data generating process for each scenario. It may reflect a single violation or a combination of violations, depending on the parameter configuration. The full specification of these synthetic models is provided in Appendix A.

Because the training data are generated under a range of violation settings, the resulting model performance can naturally be evaluated separately for different types of departures.

To further clarify the distinction between the “true” $\hat{D}$ and the predicted $\hat{D}$, we have added the following sentence to Section 7:

> “The computer vision model was trained on paired data consisting of an RGB residual plot image as input and the corresponding target value $D$, with the objective of learning to estimate $\hat{D}$.”

Here, $D$ denotes the distance computed from the known $P$ and $Q$ under the synthetic data generating process, while the computer vision model produces an estimate $\hat{D}$ from the residual plot image.

::: reviewer


11. There is no numbering for your equations.

:::


We are ignoring this comment because the appropriate equation numbering has been done. The equation numbering follows the author guidelines: where an equation is referred to in the text it is numbered, and not otherwise. The reviewer makes reference to some equation numbers.  
